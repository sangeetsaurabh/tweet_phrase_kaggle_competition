{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transform_experiments.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNB/lkuR8UNhZ7skhMmsSVD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangeetsaurabh/tweet_sentiment_extraction/blob/master/transform_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWshbOblbRNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXNJOGT3sWbQ",
        "colab_type": "text"
      },
      "source": [
        "##### Mount the Google drive for input/output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1A4RNcydk7g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee9ea906-79ce-47ba-8650-39b88ee953ea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibWYPmIyffqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = \"/content/drive/My Drive/tweet-sentiment-extraction/data\"\n",
        "tmp_folder = '/tmp'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLhrN4PxbfLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Copy the dataset\n",
        "train_data = pd.read_csv(data_folder + \"/train.csv\")\n",
        "test_data = pd.read_csv(data_folder + \"/test.csv\")\n",
        "\n",
        "#### copying the original data just to see what original text was as we update the data\n",
        "train_data[\"old_text\"] = train_data.text\n",
        "train_data[\"old_sel_text\"] = train_data.selected_text\n",
        "\n",
        "test_data[\"old_text\"] = test_data.text\n",
        "\n",
        "### Drop the data with null value\n",
        "train_data = train_data[train_data.text == train_data.text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoUDHnE5cA4n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1e797b09-d81b-4aab-ab6f-789ec7d4bac9"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>old_text</th>\n",
              "      <th>old_sel_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ...                         old_sel_text\n",
              "0  cb774db0d1  ...  I`d have responded, if I were going\n",
              "1  549e992a42  ...                             Sooo SAD\n",
              "2  088c60f138  ...                          bullying me\n",
              "3  9642c003ef  ...                       leave me alone\n",
              "4  358bd9e861  ...                        Sons of ****,\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu6mQc06e_6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### clean up following characters\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "### impacting 1 sentence\n",
        "train_data['text'] = train_data['text'].str.replace('\\xa0',' ')\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace('\\xa0',' ')\n",
        "\n",
        "\n",
        "### Impacing 2 sentences\n",
        "train_data['text'] = train_data['text'].str.replace('Â',' ')\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace('Â',' ')\n",
        "\n",
        "### Impacing 5 sentences\n",
        "train_data['text'] = train_data['text'].str.replace('``',' ')\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace('``',' ')\n",
        "\n",
        "### Impacing 1 sentence\n",
        "train_data['text'] = train_data['text'].str.replace('´','`')\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace('´','`')\n",
        "\n",
        "### Impacing 1 sentence\n",
        "train_data['text'] = train_data['text'].str.replace('\\t','')\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace('\\t','')\n",
        "\n",
        "### Impacing 155 sentences\n",
        "train_data['text'] = train_data['text'].str.replace('ï¿½',' ')\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace('ï¿½',' ')\n",
        "\n",
        "###Impacting 1220 rows (only 5 of them are in selected_text) (removing all the URLs)\n",
        "train_data['text'] = train_data['text'].str.replace(r'http\\S+', 'URL', regex=True)\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace(r'http\\S+', 'URL', regex=True)\n",
        "\n",
        "#### Impacting 2 sentects. It seems to be a bug in the data, text doesn't have those characters \n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace(r'ï', '', regex=True)\n",
        "\n",
        "#### Impacting one sentence (no impact on test data, so just ok to remove it)\n",
        "####Impacting 8 sentences. Most of them are neutral. Generally if a sentence has this, include in selected text\n",
        "train_data['text'] = train_data['text'].str.replace('\\(:\\|', '')\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace('\\(:\\|', '')\n",
        "\n",
        "\n",
        "####Impacting 8 sentences. Most of them are neutral. Generally if a sentence has this, include in selected text\n",
        "train_data['text'] = train_data['text'].str.replace(':\\|', '')\n",
        "train_data['text'] = train_data['text'].str.replace(':-\\|', '')\n",
        "\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace(':\\|', '')\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace(':-\\|', '')\n",
        "\n",
        " \n",
        "\n",
        "#### Remove some of the special characters emojis (impacts 5 to 7 rows)\n",
        "train_data['text'] = train_data['text'].str.replace(':\\*\\*-\\(', '')\n",
        "train_data['text'] = train_data['text'].str.replace(':\\*\\*\\(', '')\n",
        "train_data['text'] = train_data['text'].str.replace(':\\*', '')\n",
        "\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace(':\\*\\*-\\(', '')\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace(':\\*\\*\\(', '')\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace(':\\*', '')\n",
        "\n",
        "####Impacting 20 sentences. Most of them are neutral. Generally if a sentence has this, include in selected text\n",
        "train_data['text'] = train_data['text'].str.replace(':O', '')\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace(':O', '')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###Remove leading and trailing spaces\n",
        "train_data['text'] = train_data['text'].str.strip()\n",
        "train_data['selected_text'] = train_data['selected_text'].str.strip()\n",
        "\n",
        "train_data['text'] = train_data['text'].str.replace(r'\\s{2,}', ' ',regex=True)\n",
        "train_data['selected_text'] = train_data['selected_text'].str.replace(r'\\s{2,}', ' ',regex=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Impacing 1 sentence\n",
        "test_data['text'] = test_data['text'].str.replace('Â',' ')\n",
        "\n",
        "### Impacing 1 sentence\n",
        "test_data['text'] = test_data['text'].str.replace('``',' ')\n",
        "\n",
        "\n",
        "### Impacing 1 sentence\n",
        "test_data['text'] = test_data['text'].str.replace('\\t',' ')\n",
        "\n",
        "### Impacing 13 sentences\n",
        "test_data['text'] = test_data['text'].str.replace('ï¿½',' ')\n",
        "\n",
        "### Impacting 174 rows (removing all the URLs)\n",
        "test_data['text'] = test_data['text'].str.replace(r'http\\S+', 'URL', regex=True)\n",
        "\n",
        "### Impacting 2 rows. Just include this in the output\n",
        "test_data['text'] = test_data['text'].str.replace(':\\|', '')\n",
        "test_data['text'] = test_data['text'].str.replace(':-\\|', '')\n",
        "\n",
        "\n",
        "### Impacting 1 row. Just include this in the output\n",
        "test_data['text'] = test_data['text'].str.replace('¡', '')\n",
        "\n",
        "#### Remove some of the special characters emojis (impacts 1 row)\n",
        "test_data['text'] = test_data['text'].str.replace(':\\*', '')\n",
        "\n",
        "#### Remove some of the special characters emojis (impacts 3 rows)\n",
        "test_data['text'] = test_data['text'].str.replace(':O', '')\n",
        "\n",
        "\n",
        "###Remove leading, trailing and duplicate spaces\n",
        "test_data['text'] = test_data['text'].str.strip()\n",
        "test_data['text'] = test_data['text'].str.replace(r'\\s{2,}', ' ',regex=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTKOUVPEoAcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Some words are partially copied in selected text. This function identifies such word and updates the selected text.\n",
        "def update_partial_token(row):\n",
        "  return_row = row[\"sel_text_token\"].copy()\n",
        "  if len(row[\"set_diff\"]) > 0:\n",
        "    for item in row[\"set_diff\"]:\n",
        "      actual_text = [s for s in row[\"text_token\"] if item in s]\n",
        "      if len(actual_text) == 1:\n",
        "        sel_txt_indx = return_row.index(item)\n",
        "        return_row[sel_txt_indx] = actual_text[0]\n",
        "      else:\n",
        "        return_row = list(filter((item).__ne__,return_row))\n",
        "  return return_row\n",
        "\n",
        "        \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXzFk-mCqHJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub = \">\"\n",
        "#test_data[\"Indexes\"]= test_data[\"text\"].str.find(sub)\n",
        "#X = test_data[(test_data.Indexes != -1)]\n",
        "#sub =  r\"[\\w'`]+|[.,!?;\\^_={}\\(\\)*:<>\\d]+\"\n",
        "sub =  r\"[\\w]+|[\\d.#$%&+-/@,!?;\\^_={}\\(\\)*:<>\\d]+\"\n",
        "\n",
        "#X[\"text_token\"]= X[\"text\"].str.findall(sub)\n",
        "\n",
        "#X[(X.Indexes != -1)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFj8kOWqdJK0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 733
        },
        "outputId": "a24cd6af-bde1-47d7-cf2d-3bc4177da60d"
      },
      "source": [
        "### drop these characters {'\\t', '\\xa0', '´', '̂'}\n",
        "\n",
        "sub =  r\"[\\w'`]+|[\\d.#$%&+-/@,!?;\\^_={}\\(\\)*:<>\\d]+\"\n",
        "\n",
        "train_data[\"text\"] = train_data[\"text\"].str.lower()\n",
        "train_data[\"selected_text\"] = train_data[\"selected_text\"].str.lower()\n",
        "\n",
        "#### creating the token for text separating punctuation and alpha\n",
        "train_data[\"text_token\"]= train_data[\"text\"].str.findall(sub)\n",
        "train_data[\"sel_text_token\"]= train_data[\"selected_text\"].str.findall(sub)\n",
        "\n",
        "#### Many of the words are copied incomplete into selected text. Finding out all the scenarios when tokens are different\n",
        "train_data[\"set_diff\"] = train_data[\"sel_text_token\"].apply(set) - train_data[\"text_token\"].apply(set)\n",
        "\n",
        "train_data[\"final_sel_text_token\"] = train_data.apply(update_partial_token,axis=1)\n",
        "\n",
        "#### Correct some of the rows manually\n",
        "train_data.loc[11808,'final_sel_text_token'] = ['hate']\n",
        "train_data.loc[14172,'final_sel_text_token'] = ['bunny']\n",
        "train_data.loc[24929,'final_sel_text_token'] = ['wow']\n",
        "train_data.loc[25691,'final_sel_text_token'] = ['cheshire_cat_']\n",
        "\n",
        "train_data[\"final_set_diff\"] = train_data[\"final_sel_text_token\"].apply(set) - train_data[\"text_token\"].apply(set)\n",
        "\n",
        "train_data[[\"text\",\"text_token\",\"selected_text\",\"sel_text_token\",\"set_diff\",\"final_set_diff\",\"final_sel_text_token\"]].tail(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_token</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sel_text_token</th>\n",
              "      <th>set_diff</th>\n",
              "      <th>final_set_diff</th>\n",
              "      <th>final_sel_text_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27471</th>\n",
              "      <td>i`m defying gravity. and nobody in alll of oz, no wizard that there is or was, is ever gonna bring me down</td>\n",
              "      <td>[i`m, defying, gravity, ., and, nobody, in, alll, of, oz, ,, no, wizard, that, there, is, or, was, ,, is, ever, gonna, bring, me, down]</td>\n",
              "      <td>i`m defying gravity. and nobody in alll of oz, no wizard that there is or was, is ever gonna bring me down</td>\n",
              "      <td>[i`m, defying, gravity, ., and, nobody, in, alll, of, oz, ,, no, wizard, that, there, is, or, was, ,, is, ever, gonna, bring, me, down]</td>\n",
              "      <td>{}</td>\n",
              "      <td>{}</td>\n",
              "      <td>[i`m, defying, gravity, ., and, nobody, in, alll, of, oz, ,, no, wizard, that, there, is, or, was, ,, is, ever, gonna, bring, me, down]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27472</th>\n",
              "      <td>url - wanted to visit the animals but we were too late</td>\n",
              "      <td>[url, -, wanted, to, visit, the, animals, but, we, were, too, late]</td>\n",
              "      <td>were too late</td>\n",
              "      <td>[were, too, late]</td>\n",
              "      <td>{}</td>\n",
              "      <td>{}</td>\n",
              "      <td>[were, too, late]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27473</th>\n",
              "      <td>in spoke to you yesterday and u didnt respond girl wassup though!</td>\n",
              "      <td>[in, spoke, to, you, yesterday, and, u, didnt, respond, girl, wassup, though, !]</td>\n",
              "      <td>in spoke to you yesterday and u didnt respond girl wassup though!</td>\n",
              "      <td>[in, spoke, to, you, yesterday, and, u, didnt, respond, girl, wassup, though, !]</td>\n",
              "      <td>{}</td>\n",
              "      <td>{}</td>\n",
              "      <td>[in, spoke, to, you, yesterday, and, u, didnt, respond, girl, wassup, though, !]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27474</th>\n",
              "      <td>so i get up early and i feel good about the day. i walk to work and i`m feeling alright. but guess what... i don`t work today.</td>\n",
              "      <td>[so, i, get, up, early, and, i, feel, good, about, the, day, ., i, walk, to, work, and, i`m, feeling, alright, ., but, guess, what, ..., i, don`t, work, today, .]</td>\n",
              "      <td>i feel good ab</td>\n",
              "      <td>[i, feel, good, ab]</td>\n",
              "      <td>{ab}</td>\n",
              "      <td>{}</td>\n",
              "      <td>[i, feel, good, about]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27475</th>\n",
              "      <td>enjoy ur night</td>\n",
              "      <td>[enjoy, ur, night]</td>\n",
              "      <td>enjoy</td>\n",
              "      <td>[enjoy]</td>\n",
              "      <td>{}</td>\n",
              "      <td>{}</td>\n",
              "      <td>[enjoy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27476</th>\n",
              "      <td>wish we could come see u on denver husband lost his job and can`t afford it</td>\n",
              "      <td>[wish, we, could, come, see, u, on, denver, husband, lost, his, job, and, can`t, afford, it]</td>\n",
              "      <td>d lost</td>\n",
              "      <td>[d, lost]</td>\n",
              "      <td>{d}</td>\n",
              "      <td>{}</td>\n",
              "      <td>[lost]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27477</th>\n",
              "      <td>i`ve wondered about rake to. the client has made it clear .net only, don`t force devs to learn a new lang #agile #ccnet</td>\n",
              "      <td>[i`ve, wondered, about, rake, to, ., the, client, has, made, it, clear, ., net, only, ,, don`t, force, devs, to, learn, a, new, lang, #, agile, #, ccnet]</td>\n",
              "      <td>, don`t force</td>\n",
              "      <td>[,, don`t, force]</td>\n",
              "      <td>{}</td>\n",
              "      <td>{}</td>\n",
              "      <td>[,, don`t, force]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27478</th>\n",
              "      <td>yay good for both of you. enjoy the break - you probably need it after such hectic weekend take care hun xxxx</td>\n",
              "      <td>[yay, good, for, both, of, you, ., enjoy, the, break, -, you, probably, need, it, after, such, hectic, weekend, take, care, hun, xxxx]</td>\n",
              "      <td>yay good for both of you.</td>\n",
              "      <td>[yay, good, for, both, of, you, .]</td>\n",
              "      <td>{}</td>\n",
              "      <td>{}</td>\n",
              "      <td>[yay, good, for, both, of, you, .]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27479</th>\n",
              "      <td>but it was worth it ****.</td>\n",
              "      <td>[but, it, was, worth, it, ****.]</td>\n",
              "      <td>but it was worth it ****.</td>\n",
              "      <td>[but, it, was, worth, it, ****.]</td>\n",
              "      <td>{}</td>\n",
              "      <td>{}</td>\n",
              "      <td>[but, it, was, worth, it, ****.]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27480</th>\n",
              "      <td>all this flirting going on - the atg smiles. yay. ((hugs))</td>\n",
              "      <td>[all, this, flirting, going, on, -, the, atg, smiles, ., yay, ., ((, hugs, ))]</td>\n",
              "      <td>all this flirting going on - the atg smiles. yay. ((hugs)</td>\n",
              "      <td>[all, this, flirting, going, on, -, the, atg, smiles, ., yay, ., ((, hugs, )]</td>\n",
              "      <td>{)}</td>\n",
              "      <td>{}</td>\n",
              "      <td>[all, this, flirting, going, on, -, the, atg, smiles, ., yay, ., ((, hugs, ))]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                 text  ...                                                                                                                     final_sel_text_token\n",
              "27471                      i`m defying gravity. and nobody in alll of oz, no wizard that there is or was, is ever gonna bring me down  ...  [i`m, defying, gravity, ., and, nobody, in, alll, of, oz, ,, no, wizard, that, there, is, or, was, ,, is, ever, gonna, bring, me, down]\n",
              "27472                                                                          url - wanted to visit the animals but we were too late  ...                                                                                                                        [were, too, late]\n",
              "27473                                                               in spoke to you yesterday and u didnt respond girl wassup though!  ...                                                         [in, spoke, to, you, yesterday, and, u, didnt, respond, girl, wassup, though, !]\n",
              "27474  so i get up early and i feel good about the day. i walk to work and i`m feeling alright. but guess what... i don`t work today.  ...                                                                                                                   [i, feel, good, about]\n",
              "27475                                                                                                                  enjoy ur night  ...                                                                                                                                  [enjoy]\n",
              "27476                                                     wish we could come see u on denver husband lost his job and can`t afford it  ...                                                                                                                                   [lost]\n",
              "27477         i`ve wondered about rake to. the client has made it clear .net only, don`t force devs to learn a new lang #agile #ccnet  ...                                                                                                                        [,, don`t, force]\n",
              "27478                   yay good for both of you. enjoy the break - you probably need it after such hectic weekend take care hun xxxx  ...                                                                                                       [yay, good, for, both, of, you, .]\n",
              "27479                                                                                                       but it was worth it ****.  ...                                                                                                         [but, it, was, worth, it, ****.]\n",
              "27480                                                                      all this flirting going on - the atg smiles. yay. ((hugs))  ...                                                           [all, this, flirting, going, on, -, the, atg, smiles, ., yay, ., ((, hugs, ))]\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqQOb94AUADR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "0a260f34-4c44-4f3d-c710-d3a371b90297"
      },
      "source": [
        "train_data[train_data['final_set_diff'].apply(len) > 0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>old_text</th>\n",
              "      <th>old_sel_text</th>\n",
              "      <th>text_token</th>\n",
              "      <th>sel_text_token</th>\n",
              "      <th>set_diff</th>\n",
              "      <th>final_sel_text_token</th>\n",
              "      <th>final_set_diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [textID, text, selected_text, sentiment, old_text, old_sel_text, text_token, sel_text_token, set_diff, final_sel_text_token, final_set_diff]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1a5isq5XG6r",
        "colab_type": "text"
      },
      "source": [
        "So the above made sure that there is no difference in token between of selected text and text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ3To_v1Xlw0",
        "colab_type": "text"
      },
      "source": [
        "#### Difference in Test tokens and Training Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjyuiqXSeyJO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "23a49b3b-97a1-4b21-c206-c1deb93ef390"
      },
      "source": [
        "pd.set_option(\"display.max_rows\",500)\n",
        "\n",
        "sub =  r\"[\\w]+|[\\d.#$%&+-/@,!?;\\^_={}\\(\\)*:<>\\d]+\"\n",
        "test_data[\"text\"] = test_data[\"text\"].str.lower()\n",
        "test_data[\"text_token\"]= test_data[\"text\"].str.findall(sub)\n",
        "\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>old_text</th>\n",
              "      <th>text_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f87dea47db</td>\n",
              "      <td>last session of the day url</td>\n",
              "      <td>neutral</td>\n",
              "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
              "      <td>[last, session, of, the, day, url]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>96d74cb729</td>\n",
              "      <td>shanghai is also really exciting (precisely -- skyscrapers galore). good tweeps in china: (sh) (bj).</td>\n",
              "      <td>positive</td>\n",
              "      <td>Shanghai is also really exciting (precisely -- skyscrapers galore). Good tweeps in China:  (SH)  (BJ).</td>\n",
              "      <td>[shanghai, is, also, really, exciting, (, precisely, --, skyscrapers, galore, )., good, tweeps, in, china, :, (, sh, ), (, bj, ).]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eee518ae67</td>\n",
              "      <td>recession hit veronique branquinho, she has to quit her company, such a shame!</td>\n",
              "      <td>negative</td>\n",
              "      <td>Recession hit Veronique Branquinho, she has to quit her company, such a shame!</td>\n",
              "      <td>[recession, hit, veronique, branquinho, ,, she, has, to, quit, her, company, ,, such, a, shame, !]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>01082688c6</td>\n",
              "      <td>happy bday!</td>\n",
              "      <td>positive</td>\n",
              "      <td>happy bday!</td>\n",
              "      <td>[happy, bday, !]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>33987a8ee5</td>\n",
              "      <td>url - i like it!!</td>\n",
              "      <td>positive</td>\n",
              "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
              "      <td>[url, -, i, like, it, !!]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ...                                                                                                                          text_token\n",
              "0  f87dea47db  ...                                                                                                  [last, session, of, the, day, url]\n",
              "1  96d74cb729  ...  [shanghai, is, also, really, exciting, (, precisely, --, skyscrapers, galore, )., good, tweeps, in, china, :, (, sh, ), (, bj, ).]\n",
              "2  eee518ae67  ...                                  [recession, hit, veronique, branquinho, ,, she, has, to, quit, her, company, ,, such, a, shame, !]\n",
              "3  01082688c6  ...                                                                                                                    [happy, bday, !]\n",
              "4  33987a8ee5  ...                                                                                                           [url, -, i, like, it, !!]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p33N2IQ7SMmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_train_diff = set(test_data[\"text_token\"].sum()) - set(train_data[\"text_token\"].sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ086wowinsS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0cbe54c-3ef5-4abd-f0d2-adde2e49afc8"
      },
      "source": [
        "len(test_train_diff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1929"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYX6qF9pYFob",
        "colab_type": "text"
      },
      "source": [
        "So, there are 1929 words in Test tweets that's not there in training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtRNI20BSu_S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7f610ea3-0e4b-459f-d24a-3a266c67bb64"
      },
      "source": [
        "test_train_diff"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lowkey',\n",
              " 'hedro',\n",
              " 'aarg',\n",
              " 'impersonator',\n",
              " 'cuzin',\n",
              " 'tunage',\n",
              " 'judgements',\n",
              " 'durrin',\n",
              " 'apostro',\n",
              " 't0g0',\n",
              " 'phir',\n",
              " 'ssn',\n",
              " 'qc',\n",
              " '_adel',\n",
              " 'loos',\n",
              " 'suuuppper',\n",
              " 'interior',\n",
              " 'tda',\n",
              " 'scattegories',\n",
              " 'prospects',\n",
              " 'customized',\n",
              " 'barrels',\n",
              " 'sorronda',\n",
              " 'higherscore',\n",
              " 'sirs',\n",
              " 'grandaughter',\n",
              " 'shadez',\n",
              " 'scr',\n",
              " 'jolla',\n",
              " 'downwards',\n",
              " 'embarrass',\n",
              " 'weapons',\n",
              " 'trafficjam',\n",
              " 'reconsituted',\n",
              " 'woohooo',\n",
              " 'weirton',\n",
              " 'butmy',\n",
              " 'margeritas',\n",
              " 'ruddy',\n",
              " 'slaughter',\n",
              " 'hignfy',\n",
              " 'soccergame',\n",
              " '&5',\n",
              " 'hypocrite',\n",
              " 'stickers',\n",
              " 'biteeee',\n",
              " 'alist',\n",
              " 'medicated',\n",
              " 'highs',\n",
              " 'breton',\n",
              " 'hyperactive',\n",
              " 'casually',\n",
              " 'viewable',\n",
              " '_ember',\n",
              " '30s',\n",
              " 'wontt',\n",
              " '$20.00',\n",
              " 'chrystina',\n",
              " 'zimmermann',\n",
              " 'greeeeeeaaaaatttttttt',\n",
              " 'joel',\n",
              " 'watcing',\n",
              " '#6.',\n",
              " 'classs',\n",
              " 'controller',\n",
              " 'bork',\n",
              " 'mommyyy',\n",
              " '981',\n",
              " 'guerilla',\n",
              " '_hameron',\n",
              " 'didddd',\n",
              " 'aldi',\n",
              " 'elegant',\n",
              " 'dhellohannah5',\n",
              " 'bouvier',\n",
              " 'snapp',\n",
              " 'mixture',\n",
              " 'mondayyy',\n",
              " 'grammer',\n",
              " 'sleeeepp',\n",
              " 'testrun',\n",
              " 'everest',\n",
              " 'hearsay',\n",
              " 'ful',\n",
              " 'msged',\n",
              " 'wahooooooo',\n",
              " 'motar',\n",
              " 'peacocks',\n",
              " 'zilch',\n",
              " 'danwtmoon',\n",
              " '_18',\n",
              " 'y_spitta',\n",
              " 'kobold',\n",
              " 'gurlie',\n",
              " 'jandy',\n",
              " 'jayden',\n",
              " 'transtelecom',\n",
              " '.00.',\n",
              " 'cokey',\n",
              " 'michou',\n",
              " '15min',\n",
              " 'weber',\n",
              " 'screencasts',\n",
              " 'dover',\n",
              " 'berwick',\n",
              " 'tippy',\n",
              " 'saviness',\n",
              " '_oyler',\n",
              " 'revent',\n",
              " 'kirsty',\n",
              " 'fancast',\n",
              " 'clang',\n",
              " 'locket',\n",
              " 'bong',\n",
              " 'ahahahahahahahahah',\n",
              " 'inviataion',\n",
              " 'herb',\n",
              " 'mick',\n",
              " 'stakes',\n",
              " 'transmitter',\n",
              " 'robban',\n",
              " 'coats',\n",
              " 'catacombs',\n",
              " 'shortchanged',\n",
              " '$38',\n",
              " 'ennio',\n",
              " 'strum',\n",
              " 'ahahha',\n",
              " 'jacob',\n",
              " 'laundromat',\n",
              " '_serafina85',\n",
              " 'notified',\n",
              " 'shortcuts',\n",
              " 'japa',\n",
              " 'd3ff',\n",
              " 'abandon',\n",
              " 'tofu',\n",
              " 'uli',\n",
              " '9412',\n",
              " 'lovess',\n",
              " '1997',\n",
              " 'chaz',\n",
              " 'commmmmiinnnnnggggggg',\n",
              " '_iddon',\n",
              " 'dono',\n",
              " '(10',\n",
              " 'twitterloves',\n",
              " 'headset',\n",
              " 'insult',\n",
              " 'inorite',\n",
              " 'fuunny',\n",
              " 'blockkkkk',\n",
              " 'n33d',\n",
              " 'tlkin',\n",
              " 'condre',\n",
              " 'suppossed',\n",
              " 'bradbury',\n",
              " 'shaft',\n",
              " 'cpk',\n",
              " 'mirage',\n",
              " 'prednisone',\n",
              " '7mth',\n",
              " 'the_wockeez',\n",
              " '_sutra',\n",
              " '_beag',\n",
              " 'burner',\n",
              " 'nvrmind',\n",
              " 'risks',\n",
              " 'mayne',\n",
              " 'vstudio',\n",
              " 'actualy',\n",
              " 'klein',\n",
              " 'bejewled',\n",
              " 'structured',\n",
              " 'honoured',\n",
              " 'kacy',\n",
              " 'criminy',\n",
              " 'wendyhouse',\n",
              " '28deg',\n",
              " 'receipts',\n",
              " 'therefor',\n",
              " 'sinfest',\n",
              " 'applicants',\n",
              " ':16',\n",
              " '120mm',\n",
              " 'hecks',\n",
              " 'tanapolis',\n",
              " 'dp',\n",
              " 'ingrid',\n",
              " 'witht',\n",
              " 'suffocating',\n",
              " 'lifestory',\n",
              " '_ellen',\n",
              " 'brides',\n",
              " ':31',\n",
              " 'quarterly',\n",
              " 'governors',\n",
              " 'relieved',\n",
              " 'reaching',\n",
              " 'disguises',\n",
              " 'gals',\n",
              " 'punkrockchick25',\n",
              " 'iont',\n",
              " 'zimbabwe',\n",
              " 'gyaan',\n",
              " 'offers',\n",
              " 'defunk',\n",
              " 'lighters',\n",
              " 'codeh',\n",
              " 'fronts',\n",
              " 'thn',\n",
              " 'utv',\n",
              " ',(',\n",
              " 'tumblarity',\n",
              " 'camiguin',\n",
              " 'ferrer',\n",
              " '_tha',\n",
              " 'hydes',\n",
              " 'sharen',\n",
              " 'blockers',\n",
              " 'searches',\n",
              " 'skateparks',\n",
              " 'vaneta',\n",
              " 'kasa',\n",
              " 'alignment',\n",
              " 'portmeirion',\n",
              " 'vampires',\n",
              " 'gday',\n",
              " 'photographing',\n",
              " 'ksn',\n",
              " 'aaass',\n",
              " 'carnage',\n",
              " 'sinai',\n",
              " 'ashleyyyy',\n",
              " 'mahjong',\n",
              " 'succesfully',\n",
              " 'mississippi',\n",
              " '_flo',\n",
              " 'veoh',\n",
              " 'collectibles',\n",
              " 'longshot',\n",
              " 'freud',\n",
              " 'speachless',\n",
              " 'dibbs',\n",
              " 'hernia',\n",
              " '_fur_laxis',\n",
              " 'rigth',\n",
              " 'scarepoint',\n",
              " '-------->>>>>',\n",
              " 'happenings',\n",
              " 'boya',\n",
              " 'chirstina',\n",
              " 'butte',\n",
              " 'gory',\n",
              " 'moth',\n",
              " '_andersen',\n",
              " 'gram',\n",
              " 'realtor',\n",
              " 'bathgate',\n",
              " 'tonic',\n",
              " 'coughin',\n",
              " 'compilation',\n",
              " 'have2cut',\n",
              " '-5,',\n",
              " 'handwriting',\n",
              " '!--',\n",
              " 'vaders',\n",
              " 'arou',\n",
              " ':09',\n",
              " 'lasttt',\n",
              " '_hodges',\n",
              " 'compelte',\n",
              " 'songwriter',\n",
              " 'cuase',\n",
              " 'icof',\n",
              " 'ratarsed',\n",
              " 'bbs',\n",
              " 'geno',\n",
              " 'girlish',\n",
              " '.24',\n",
              " 'rasberries',\n",
              " 'cervelo',\n",
              " 'disown',\n",
              " 'glimpsing',\n",
              " 'aaaaa',\n",
              " 'clonecloud',\n",
              " 'blerrrrrrrrrrgh',\n",
              " 'tweat',\n",
              " 'pea',\n",
              " 'synday',\n",
              " 'jks',\n",
              " 'sameperson',\n",
              " 'limbo',\n",
              " 'operated',\n",
              " 'dx',\n",
              " 'lea',\n",
              " 'backkk',\n",
              " 'hottt',\n",
              " 'hindu',\n",
              " 'exercises',\n",
              " 'warrier',\n",
              " 'childish',\n",
              " 'unicorn',\n",
              " '$3,500',\n",
              " 'whb',\n",
              " 'mumborg',\n",
              " 'brags',\n",
              " 'concord',\n",
              " 'isang',\n",
              " 'provisioning',\n",
              " 'braddddddddd',\n",
              " 'esti',\n",
              " 'terrell',\n",
              " 'reverse',\n",
              " 'uncommon',\n",
              " 'waaalkiiing',\n",
              " 'devastating',\n",
              " 'homenagem',\n",
              " 'unbreakable',\n",
              " 'fortified',\n",
              " 'zi',\n",
              " 'bulked',\n",
              " 'chitty',\n",
              " 'rod',\n",
              " 'valk',\n",
              " 'n00b',\n",
              " 'stalk',\n",
              " 'bcause',\n",
              " 'ptsici',\n",
              " 'hundreds',\n",
              " 'diabetic',\n",
              " 'wok',\n",
              " 'springhill',\n",
              " 'ambivalent',\n",
              " 'merci',\n",
              " 'humpthestump',\n",
              " 'cassoulet',\n",
              " 'tj',\n",
              " 'lamest',\n",
              " 'lunchfast',\n",
              " 'meadows',\n",
              " 'editeditedittt',\n",
              " '_witty',\n",
              " 'minuets',\n",
              " 'reinforce',\n",
              " 'canadamigos',\n",
              " 'turquoise',\n",
              " 'temples',\n",
              " 'opportunities',\n",
              " 'psychological',\n",
              " '!<3',\n",
              " 'marine',\n",
              " 'cottin',\n",
              " 'normail',\n",
              " 'gaby',\n",
              " 'alongs',\n",
              " 'alen',\n",
              " 'mindddd',\n",
              " 'kandi',\n",
              " 'behaviour',\n",
              " 'joemar',\n",
              " 'milling',\n",
              " 'poorpoor',\n",
              " 'absoulutely',\n",
              " '550',\n",
              " 'laze',\n",
              " '_the_bigman',\n",
              " 'backe',\n",
              " 'farking',\n",
              " 'arosh',\n",
              " 'kittie',\n",
              " 'gigolo',\n",
              " 'longeerr',\n",
              " 'bangalorean',\n",
              " 'woow',\n",
              " 'skylight',\n",
              " 'constraints',\n",
              " 'lastnite',\n",
              " 'deniseee',\n",
              " 'retching',\n",
              " '_92',\n",
              " '++',\n",
              " 'sixteen',\n",
              " 'howlin',\n",
              " 'faculty',\n",
              " 'lrc',\n",
              " 'tudors',\n",
              " 'gawww',\n",
              " 'officialshew',\n",
              " 'cooperated',\n",
              " 'src',\n",
              " 'creates',\n",
              " 'eagerly',\n",
              " 'analogy',\n",
              " 'spreads',\n",
              " 'blk',\n",
              " 'matthews',\n",
              " 'heared',\n",
              " 'pbp',\n",
              " 'bucky',\n",
              " 'littrell',\n",
              " 'calpol',\n",
              " 'kto',\n",
              " 'sharonp',\n",
              " 'grint',\n",
              " 'lollies',\n",
              " 'tition',\n",
              " 'datalounge',\n",
              " 'upssucks',\n",
              " 'itstyleryo',\n",
              " 'sumo',\n",
              " 'vibrators',\n",
              " 'grangemouth',\n",
              " '_jim',\n",
              " 'spinners',\n",
              " 'netflix',\n",
              " 'throughout',\n",
              " 'antibacterial',\n",
              " 'cheeeck',\n",
              " '(2)',\n",
              " 'hilarity',\n",
              " 'invitaiton',\n",
              " 'regularly',\n",
              " 'missss',\n",
              " 'peasants',\n",
              " 'beckons',\n",
              " 'ginoooo',\n",
              " 'dicks',\n",
              " 'eweryone',\n",
              " 'minic',\n",
              " 'horrified',\n",
              " 'weetabix',\n",
              " 'cages',\n",
              " 'illionare',\n",
              " 'kickboxing',\n",
              " 'rootin',\n",
              " 'boxed',\n",
              " 'trucks',\n",
              " 'jockey',\n",
              " 'mobileme',\n",
              " 'peeple',\n",
              " 'owned',\n",
              " 'youd',\n",
              " '_jlh',\n",
              " 'ultimatum',\n",
              " 'vocalist',\n",
              " 'blockheads',\n",
              " 'yerrrr',\n",
              " 'phela',\n",
              " 'bww',\n",
              " 'hanami',\n",
              " 'sfs',\n",
              " 'jizzle',\n",
              " 'raddest',\n",
              " 'horizon',\n",
              " 'doind',\n",
              " '<)',\n",
              " 't_t',\n",
              " 'evp',\n",
              " 'bataan',\n",
              " 'vouches',\n",
              " 'flavorus',\n",
              " 'balboa',\n",
              " 'hahaaaaaaaa',\n",
              " 'vivocity',\n",
              " 'herded',\n",
              " 'gushy',\n",
              " 'awoken',\n",
              " 'shootout',\n",
              " 'bbqs',\n",
              " 'ticketmaster',\n",
              " 'romantically',\n",
              " 'rumour',\n",
              " 'effffffort',\n",
              " '_kikireestl',\n",
              " 'fray',\n",
              " 'watership',\n",
              " '<->',\n",
              " 'straighening',\n",
              " 'pinhole',\n",
              " '_alfradique',\n",
              " 'nagustuhan',\n",
              " 'jiayou',\n",
              " 'wowie',\n",
              " 'aim6',\n",
              " 'typhoon',\n",
              " 'snowbear',\n",
              " 'whin',\n",
              " 'ffrom',\n",
              " 'todaaaay',\n",
              " 'lethargic',\n",
              " 'sleepypants',\n",
              " 'darth',\n",
              " 'burguer',\n",
              " 'geocache',\n",
              " 'sociological',\n",
              " 'tabbing',\n",
              " 'sleeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeep',\n",
              " 'modesty',\n",
              " '_preet',\n",
              " 'poking',\n",
              " 'tiiiired',\n",
              " 'g4c09',\n",
              " '_traveler',\n",
              " 'aslyum',\n",
              " ',,,,,,,,,',\n",
              " ':17',\n",
              " '_durden',\n",
              " 'hubbys',\n",
              " 'nagy',\n",
              " 'programmers',\n",
              " '$0.50',\n",
              " 'benji',\n",
              " 'kiya',\n",
              " 'yellowcard',\n",
              " 'jasmine',\n",
              " 'macro',\n",
              " 'monsoon',\n",
              " 'handlebar',\n",
              " 'wouldn',\n",
              " 'ir',\n",
              " 'lr',\n",
              " 'coney',\n",
              " 'you1',\n",
              " 'punya',\n",
              " 'hahhaha',\n",
              " ':50..',\n",
              " 'commrcials',\n",
              " 'che',\n",
              " '_itx',\n",
              " 'consti',\n",
              " 'rpt',\n",
              " 'gerardo',\n",
              " '_me',\n",
              " 'voter',\n",
              " 'tg',\n",
              " 'cajon',\n",
              " 'tweeterizing',\n",
              " 'subdivision',\n",
              " 'gcse',\n",
              " 'estonia',\n",
              " 'interruption',\n",
              " '_amr',\n",
              " 'weren',\n",
              " 'b2k',\n",
              " '_tron',\n",
              " 'grills',\n",
              " '...=',\n",
              " 'orch',\n",
              " 'fruitcake',\n",
              " 'jonesing',\n",
              " '_allison',\n",
              " 'consequences',\n",
              " 'neglecting',\n",
              " 'sympathizes',\n",
              " 'tenaya',\n",
              " 'dipping',\n",
              " 'wldn',\n",
              " 'iyo',\n",
              " 'breakthrough',\n",
              " 'drills',\n",
              " 'grrrrrrrrrrrrrr',\n",
              " 'scouts',\n",
              " 'mixtape',\n",
              " 'kiddie',\n",
              " 'schaumburg',\n",
              " 'kiddo',\n",
              " 'hasn',\n",
              " 'sleeeeping',\n",
              " 'bummerrr',\n",
              " 'wooie',\n",
              " 'sporebat',\n",
              " 'shannie',\n",
              " 'phootbooth',\n",
              " 'loc',\n",
              " 'and1',\n",
              " '_08',\n",
              " 'jodie',\n",
              " 'annuals',\n",
              " 'pate',\n",
              " '_possa',\n",
              " '_celeste',\n",
              " 'seafoods',\n",
              " 'waled',\n",
              " 'javitz',\n",
              " ';)<3',\n",
              " 'matutunaw',\n",
              " 'willen',\n",
              " 'fiddling',\n",
              " 'anticipated',\n",
              " '++.',\n",
              " 'billie',\n",
              " '4e',\n",
              " 'reduces',\n",
              " 'verge',\n",
              " 'melly',\n",
              " 'boulevard',\n",
              " 'ovr',\n",
              " 'breakdown',\n",
              " 'loooool',\n",
              " 'slay',\n",
              " 'loveyoujonesy',\n",
              " 'inooo',\n",
              " '#24?',\n",
              " 'connectivity',\n",
              " 'devotees',\n",
              " 'violence',\n",
              " 'brady',\n",
              " 'lemongrass',\n",
              " 'preoccupation',\n",
              " 'philips',\n",
              " 'favoriteicecream',\n",
              " 'becausei',\n",
              " '-1..',\n",
              " 'vishal',\n",
              " 'shri',\n",
              " 'metals',\n",
              " '_g1986',\n",
              " 'sftw',\n",
              " 'omginorite',\n",
              " 'dicking',\n",
              " 'propellerheads',\n",
              " 'relishing',\n",
              " 'rox',\n",
              " 'hrmm',\n",
              " '_43',\n",
              " 'tabard',\n",
              " 'nutreal',\n",
              " 'constance',\n",
              " 'hackers',\n",
              " 'exxagerated',\n",
              " 'scramming',\n",
              " '=_=',\n",
              " '10000',\n",
              " 'yopu',\n",
              " 'onigiri',\n",
              " '_lamont',\n",
              " 'ck',\n",
              " 'carlessness',\n",
              " 'chaaron',\n",
              " 'falcon',\n",
              " 'conviced',\n",
              " 'postrock',\n",
              " 'doneeee',\n",
              " '*!',\n",
              " 'reprieve',\n",
              " 'twitterwar',\n",
              " 'unhappiness',\n",
              " 'strand',\n",
              " 'itin',\n",
              " 'espesh',\n",
              " 'selecting',\n",
              " 'overdraft',\n",
              " 'dullful',\n",
              " 'brains',\n",
              " 'mannn',\n",
              " 'asshat',\n",
              " 'ferdinand',\n",
              " 'kuala',\n",
              " '77kilos',\n",
              " 'transmit',\n",
              " 'humming',\n",
              " 'postcards',\n",
              " 'arttt',\n",
              " 'mixin',\n",
              " 'exagerrated',\n",
              " 'tosh',\n",
              " '<3*',\n",
              " 'cartilage',\n",
              " 'nigth',\n",
              " 'pelham',\n",
              " 'toefl',\n",
              " 'obsessing',\n",
              " 'whem',\n",
              " 'divs',\n",
              " 'scenery',\n",
              " '_words',\n",
              " 'notin',\n",
              " 'formated',\n",
              " 'mardy',\n",
              " 'nieces',\n",
              " 'enyone',\n",
              " 'weeeeks',\n",
              " 'bumming',\n",
              " '(11',\n",
              " 'ike',\n",
              " 'rebooted',\n",
              " 'twappy',\n",
              " 'disturbing',\n",
              " 'tiffys',\n",
              " 'lease',\n",
              " 'tmnt',\n",
              " 'siiiiiickkk',\n",
              " 'sappin',\n",
              " 'shindig',\n",
              " '_fenton',\n",
              " 'rmbr',\n",
              " 'sensationalist',\n",
              " 'misspelled',\n",
              " 'rearry',\n",
              " 'oxymoron',\n",
              " 'yanno',\n",
              " 'shawn',\n",
              " 'doubling',\n",
              " 'feeln',\n",
              " 'ciaran',\n",
              " 'june1',\n",
              " 'envolopes',\n",
              " 'anberlin',\n",
              " 'shill',\n",
              " 'noko',\n",
              " 'abundantly',\n",
              " 'quicker',\n",
              " '_ess',\n",
              " 'twipics',\n",
              " 'farrr',\n",
              " 'lazzzzzzzzzzzzzzzy',\n",
              " 'birthdayyy',\n",
              " 'twitdroid',\n",
              " 'easiest',\n",
              " 'grrl',\n",
              " 'edgefest',\n",
              " 'practices',\n",
              " 'acum',\n",
              " 'hom',\n",
              " 'rotf',\n",
              " 'responsibly',\n",
              " 'simcity',\n",
              " '_merchant',\n",
              " '_hk',\n",
              " 'billyidol',\n",
              " 'muahaz',\n",
              " 'fantasty',\n",
              " 'lancie',\n",
              " 'ferret',\n",
              " 'nsfw',\n",
              " '59',\n",
              " 'tweetioi',\n",
              " 'leme',\n",
              " '54',\n",
              " 'rx',\n",
              " 'genetics',\n",
              " 'soonnn',\n",
              " 'kanji',\n",
              " 'awkard',\n",
              " 'rsa',\n",
              " 'reconnecting',\n",
              " 'sowey',\n",
              " 'costing',\n",
              " 'melon',\n",
              " 'district',\n",
              " 'bleargh',\n",
              " 'shivering',\n",
              " '2833',\n",
              " 'monin',\n",
              " 'championship',\n",
              " 'acronym',\n",
              " 'freestyling',\n",
              " 'breakupwords',\n",
              " 'payingitforward',\n",
              " 'yp',\n",
              " 'generous',\n",
              " 'prohibiting',\n",
              " 'goddamit',\n",
              " 'anoying',\n",
              " 'natalya',\n",
              " 'vacaville',\n",
              " 'anniston',\n",
              " 'features',\n",
              " 'reemer',\n",
              " 'fronted',\n",
              " 'photofollows',\n",
              " 'clubing',\n",
              " 'taunt',\n",
              " 'rita',\n",
              " 'woodburn',\n",
              " 'surge',\n",
              " '250g',\n",
              " '_o_c',\n",
              " 'mediteranian',\n",
              " 'yeeeewww',\n",
              " 'protecting',\n",
              " 'ncaa',\n",
              " '1970s',\n",
              " 'fawxing',\n",
              " 'kix',\n",
              " 'clog',\n",
              " '/09',\n",
              " 'reactionaries',\n",
              " 'tracy',\n",
              " 'oks',\n",
              " 'dill',\n",
              " 'n2',\n",
              " 'curtis',\n",
              " 'courto',\n",
              " 'brittneyy',\n",
              " 'sandpit',\n",
              " 'mgm',\n",
              " 'betrayed',\n",
              " 'creepers',\n",
              " 'timeeee',\n",
              " 'dlp',\n",
              " 'mkt',\n",
              " 'teatree',\n",
              " 'aaaaaaaa',\n",
              " 'baixa',\n",
              " 'jolie',\n",
              " 'sunner',\n",
              " 'sahm',\n",
              " '-___-',\n",
              " '_eat_out',\n",
              " 'twitterbones',\n",
              " 'nig',\n",
              " 'loong',\n",
              " '_liden',\n",
              " 'towers',\n",
              " 'kiddin',\n",
              " 'lulu',\n",
              " 'lifestyles',\n",
              " 'shingle',\n",
              " 'culprit',\n",
              " '_other',\n",
              " 'frolick',\n",
              " 'couches',\n",
              " 'toothpaste',\n",
              " 'shoots',\n",
              " 'arghhhhhhhhh',\n",
              " '%!',\n",
              " 'twill',\n",
              " 'winshit',\n",
              " 'exploding',\n",
              " 'beckky',\n",
              " 'denomination',\n",
              " 'instrumentalists',\n",
              " 'soderling',\n",
              " 'homerun',\n",
              " '_griffiths',\n",
              " 'overflow',\n",
              " 'thriller',\n",
              " 'stainage',\n",
              " 'rackers',\n",
              " 'sublime',\n",
              " 'snowing',\n",
              " 'baaad',\n",
              " 'cavite',\n",
              " 'funnyyy',\n",
              " 'insisted',\n",
              " 'jewellery',\n",
              " 'ghunghte',\n",
              " 'potuguese',\n",
              " 'psycology',\n",
              " 'lovelytrinkets',\n",
              " 'tays',\n",
              " 'breastfeeding',\n",
              " 'geebus',\n",
              " 'logan',\n",
              " 'wacko',\n",
              " 'tweetz',\n",
              " '2w2v',\n",
              " '6gb',\n",
              " 'numpy',\n",
              " 'ejecting',\n",
              " 'konami',\n",
              " '.11',\n",
              " 'loudd',\n",
              " 'bouvierb',\n",
              " 'raaaain',\n",
              " 'mechanical',\n",
              " 'fragment',\n",
              " 'tapan',\n",
              " 'choco',\n",
              " 'carly',\n",
              " 'facts',\n",
              " 'equaled',\n",
              " 'prescriptions',\n",
              " 'hobart',\n",
              " 'expressed',\n",
              " 'whatevs',\n",
              " '#..',\n",
              " 'sankar',\n",
              " 'poole',\n",
              " 'casue',\n",
              " 'techmeme',\n",
              " 'clydesdale',\n",
              " 'joykill',\n",
              " 'superfly',\n",
              " 'crys',\n",
              " 'arsed',\n",
              " 'nibs',\n",
              " 'medicate',\n",
              " 'hahahhah',\n",
              " '_tweets',\n",
              " 'shamefully',\n",
              " 'famo',\n",
              " 'pelt',\n",
              " 'guerrilla',\n",
              " '_bebop',\n",
              " 'oherr',\n",
              " 'saddddddd',\n",
              " 'roping',\n",
              " 'mtvawards',\n",
              " 'duets',\n",
              " 'hairband',\n",
              " 'notlooking',\n",
              " 'cask',\n",
              " 'perkinz',\n",
              " 'muahahahhahaha',\n",
              " ':15,',\n",
              " 'blaring',\n",
              " 'gmn',\n",
              " 'rotary',\n",
              " 'baju',\n",
              " 'peaky',\n",
              " 'predictive',\n",
              " '4000',\n",
              " 'seaso',\n",
              " 'care2',\n",
              " 'lifechanging',\n",
              " 'ofcmates',\n",
              " '(.....)',\n",
              " 'printchick',\n",
              " '_cal',\n",
              " 'sexiest',\n",
              " 'xkcd',\n",
              " 'weisheng',\n",
              " 'greypoupon',\n",
              " 'craaaap',\n",
              " 'superstars',\n",
              " 'outs',\n",
              " 'dagger',\n",
              " 'licensing',\n",
              " 'rehydrated',\n",
              " 'scammed',\n",
              " 'lazed',\n",
              " 'panchito',\n",
              " 'sentry',\n",
              " 'formerly',\n",
              " 'partaaay',\n",
              " 'paintball',\n",
              " 'awsum',\n",
              " 'xiuxiu',\n",
              " 'shanghai',\n",
              " 'revolutionizes',\n",
              " 'immobiliser',\n",
              " 'umtil',\n",
              " 'tweetdeck_0_25_manual_blink182',\n",
              " 'uncles',\n",
              " 'checkered',\n",
              " 'chichester',\n",
              " 'mudweight',\n",
              " '_inc',\n",
              " 'applecare',\n",
              " 'tsu',\n",
              " 'nku',\n",
              " 'arcel',\n",
              " '_buckley',\n",
              " 'leapord',\n",
              " 'vm',\n",
              " 'ffe',\n",
              " 'gooo',\n",
              " 'xcept',\n",
              " 'maccas',\n",
              " 'browsin',\n",
              " 'arrgh',\n",
              " 'spinner',\n",
              " 'komen',\n",
              " 'ujala',\n",
              " 'gawwddd',\n",
              " 'brah',\n",
              " 'lum',\n",
              " 'freya',\n",
              " 'thingsmummysaid',\n",
              " 'dwellers',\n",
              " 'wrrkk',\n",
              " 'jalepenos',\n",
              " 'homeworks',\n",
              " 'bangus',\n",
              " 'econ',\n",
              " 'hermana',\n",
              " 'kella',\n",
              " 'paps',\n",
              " 'boge',\n",
              " 'jumper',\n",
              " 'bmw',\n",
              " 'yayayyayayay',\n",
              " 'picolo',\n",
              " 'creds',\n",
              " '.1.5',\n",
              " 'svetlana',\n",
              " 'scorcher',\n",
              " 'ftrh',\n",
              " 'fixes',\n",
              " 'undervote',\n",
              " 'tuck',\n",
              " '_1001',\n",
              " 'cohen',\n",
              " '1330',\n",
              " 'conclusion',\n",
              " 'blaming',\n",
              " 'corky',\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCRU-U8reZSn",
        "colab_type": "text"
      },
      "source": [
        "A lot of words are different because they are misspelled or are alphanumeric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHweUlPzepOz",
        "colab_type": "text"
      },
      "source": [
        "#### Lemmetization\n",
        "\n",
        "Let's lemmetize them to see if that helps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCnSo3Cxe-mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW74pRpHYcve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_train_words = list(set(train_data[\"text_token\"].sum()))\n",
        "unique_lemma_train_words = []\n",
        "separator = ', '\n",
        "doc = nlp(separator.join(unique_train_words))\n",
        "for token in doc:\n",
        "  unique_lemma_train_words.append(token.lemma_)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCtRXghTeLSi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c85dc1e4-7f4a-4e16-be64-fd1c9883dac1"
      },
      "source": [
        "print (f'The number of unique words in train dataset after lemmetization {len(unique_lemma_train_words)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of unique words in train dataset after lemmetization 58904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxmOl28BZEvT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unique_test_words = list(set(test_data[\"text_token\"].sum()))\n",
        "unique_lemma_test_words = []\n",
        "separator = ', '\n",
        "doc = nlp(separator.join(unique_test_words))\n",
        "for token in doc:\n",
        "  unique_lemma_test_words.append(token.lemma_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU8P_EKKeRSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "050876b2-cb30-422e-8733-64ee212361f3"
      },
      "source": [
        "print (f'The number of unique words in train dataset after lemmetization {len(unique_lemma_test_words)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of unique words in train dataset after lemmetization 15038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcgmUatpaFcb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59fb9d1a-b2d6-419b-c443-f5567352e8d4"
      },
      "source": [
        "test_train_diff = set(unique_lemma_test_words) - set(unique_lemma_train_words)\n",
        "len(test_train_diff)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1692"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXB00v7ogBk7",
        "colab_type": "text"
      },
      "source": [
        "Still there are 1692 words that are different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLotuM3YgFJ8",
        "colab_type": "text"
      },
      "source": [
        "#### Spellchecker\n",
        "\n",
        "Let's see if fixing the spelling may help"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMT57E_SbCKC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f170b915-3d46-473f-bebe-9c5e3ffca606"
      },
      "source": [
        "!pip install pyspellchecker"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/d1/ec4e830e9f9c1fd788e1459dd09279fdf807bc7a475579fd7192450b879c/pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 2.6MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhQtbPXza8yU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()\n",
        "misspelled = spell.unknown(test_train_diff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEi0xsTAabn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spell_corrected_word = []\n",
        "\n",
        "word_count = 0\n",
        "for word in misspelled:\n",
        "  spell_corrected_word.append(spell.correction(word))\n",
        "  #print(word, spell.correction(word))\n",
        "  #print(spell.candidates(word))\n",
        "  #word_count += 1\n",
        "  #if word_count > 10:\n",
        "  #  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iINqxdCb4BJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa9e2c10-f3bc-4ff0-8b5a-cc58e9811991"
      },
      "source": [
        "len(set(spell_corrected_word) - set(unique_lemma_train_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "523"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VMQ0aNpipYR",
        "colab_type": "text"
      },
      "source": [
        "After spelling correction, there are 523 words that are different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEhxePcEdlLh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b6a9ea8-7094-4278-f5ea-49d7299566f4"
      },
      "source": [
        "set(spell_corrected_word) - set(unique_lemma_train_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'--------',\n",
              " '-----------',\n",
              " '-___-',\n",
              " '..............',\n",
              " '1970s',\n",
              " '30/6:45-',\n",
              " '5000th',\n",
              " '^)<^',\n",
              " 'a209',\n",
              " 'aaaaaaaa',\n",
              " 'abou',\n",
              " 'absorb',\n",
              " 'after-school',\n",
              " 'agatha',\n",
              " 'ahahahahahahahahah',\n",
              " 'air-conditioned',\n",
              " 'aksum',\n",
              " 'alfradique',\n",
              " 'aloft',\n",
              " 'amass',\n",
              " 'annandale',\n",
              " 'anode',\n",
              " 'apostle',\n",
              " 'appin',\n",
              " 'applecart',\n",
              " 'are',\n",
              " 'arghhhhhhhhh',\n",
              " 'arose',\n",
              " 'ashleyyyy',\n",
              " 'aura',\n",
              " 'b2',\n",
              " 'babylon',\n",
              " 'baku',\n",
              " 'banknote',\n",
              " 'barro',\n",
              " 'based',\n",
              " 'battens',\n",
              " 'bawling',\n",
              " 'bbs',\n",
              " 'been',\n",
              " 'bells',\n",
              " 'billyidol',\n",
              " 'biteeee',\n",
              " 'blerrrrrrrrrrgh',\n",
              " 'blockkkkk',\n",
              " 'blogtalk',\n",
              " 'bloooooow',\n",
              " 'boardgame',\n",
              " 'bolan',\n",
              " 'bolton',\n",
              " 'boogie',\n",
              " 'bookcase',\n",
              " 'boooooooooo',\n",
              " 'borland',\n",
              " 'bostock',\n",
              " 'bothy',\n",
              " 'boto',\n",
              " 'boxes',\n",
              " 'braddddddddd',\n",
              " 'branquinho',\n",
              " 'breakupword',\n",
              " 'brittneyy',\n",
              " 'brrrr',\n",
              " 'bumface',\n",
              " 'bupa',\n",
              " 'bushy',\n",
              " 'buttressing',\n",
              " 'butty',\n",
              " 'caixa',\n",
              " 'caladesi',\n",
              " 'camiguin',\n",
              " 'canadamigo',\n",
              " 'canton',\n",
              " 'carelessness',\n",
              " 'cassoulet',\n",
              " 'catacombs',\n",
              " 'cerezo',\n",
              " 'chaayaa',\n",
              " 'chandra',\n",
              " 'charon',\n",
              " 'checks',\n",
              " 'chilli',\n",
              " 'cl100',\n",
              " 'clonecloud',\n",
              " 'cnaa',\n",
              " 'comic-strip',\n",
              " 'coming',\n",
              " 'commis',\n",
              " 'commmmmiinnnnnggggggg',\n",
              " 'conde',\n",
              " 'coniston',\n",
              " 'copycat',\n",
              " 'corruptedangel',\n",
              " 'covergirl',\n",
              " 'craaaap',\n",
              " 'danwtmoon',\n",
              " 'datalounge',\n",
              " 'debunk',\n",
              " 'dehydrated',\n",
              " 'delirium',\n",
              " 'dhani',\n",
              " 'dhellohannah5',\n",
              " 'diddle',\n",
              " 'diehl',\n",
              " 'dies',\n",
              " 'dirrrrrty',\n",
              " 'doing',\n",
              " 'done',\n",
              " 'donee',\n",
              " 'donnelly',\n",
              " 'dreamy',\n",
              " 'durkin',\n",
              " 'dutiful',\n",
              " 'earnout',\n",
              " 'edelcrie',\n",
              " 'editeditedittt',\n",
              " 'eee-eee',\n",
              " 'effffffort',\n",
              " 'elise',\n",
              " 'elspeth',\n",
              " 'emilyyyyyyyy',\n",
              " 'envisioning',\n",
              " 'est.',\n",
              " 'estella',\n",
              " 'ethereal',\n",
              " 'ex-pat',\n",
              " 'exaggerate',\n",
              " 'f9',\n",
              " 'fans',\n",
              " 'farr',\n",
              " 'favoriteicecream',\n",
              " 'faxing',\n",
              " 'ferret',\n",
              " 'firelight',\n",
              " 'firms',\n",
              " 'flavours',\n",
              " 'followerzzz',\n",
              " 'foretelling',\n",
              " 'forgottenmost',\n",
              " 'fshionwk',\n",
              " 'fur_laxis',\n",
              " 'fyaaahhh',\n",
              " 'g1986',\n",
              " 'g40',\n",
              " 'gatlinburg',\n",
              " 'gawwddd',\n",
              " 'geese',\n",
              " 'getting',\n",
              " 'ghunghte',\n",
              " 'ginoooo',\n",
              " 'glenn',\n",
              " 'goddammit',\n",
              " 'goddard',\n",
              " 'goodmorne',\n",
              " 'gouache',\n",
              " 'granddaughter',\n",
              " 'graverobber',\n",
              " 'greeattt',\n",
              " 'greeeeeeaaaaatttttttt',\n",
              " 'greypoupon',\n",
              " 'grounded',\n",
              " 'grrrrrreeeeeaat',\n",
              " 'grrrrrrrrrrrrrr',\n",
              " 'grrrsville',\n",
              " 'guestroom',\n",
              " 'had',\n",
              " 'hahaaaaaaaa',\n",
              " 'hahahahahahahaha',\n",
              " 'hahahhah',\n",
              " 'hahhaha',\n",
              " 'hakami',\n",
              " 'has',\n",
              " 'hasan',\n",
              " 'have2live',\n",
              " 'hayden',\n",
              " 'he',\n",
              " 'heads',\n",
              " 'headshotss',\n",
              " 'hela',\n",
              " 'helloveggie',\n",
              " 'herr',\n",
              " 'higherscore',\n",
              " 'hirigana',\n",
              " 'homenagem',\n",
              " 'homieeeeeeeeeee',\n",
              " 'house-warming',\n",
              " 'houseofrock',\n",
              " 'howling',\n",
              " 'humpthestump',\n",
              " 'i960',\n",
              " 'iamkiara',\n",
              " 'iii',\n",
              " 'im_chris',\n",
              " 'in_a_story',\n",
              " 'indivibe',\n",
              " 'insolence',\n",
              " 'inspired',\n",
              " 'insulinoma',\n",
              " 'interfacelift',\n",
              " 'ionising',\n",
              " 'is',\n",
              " 'its',\n",
              " 'its_sarah',\n",
              " 'itstyleryo',\n",
              " 'jalepenos',\n",
              " 'javits',\n",
              " 'jessandnicolemusic',\n",
              " 'jiayou',\n",
              " 'jodami',\n",
              " 'jonas101',\n",
              " 'jpnnbak',\n",
              " 'jumped',\n",
              " 'kashan',\n",
              " 'kershaw',\n",
              " 'kickboxing',\n",
              " 'kikireestl',\n",
              " 'killumbus',\n",
              " 'kilos',\n",
              " 'kiokudb',\n",
              " 'kit-kat',\n",
              " 'lastnite',\n",
              " 'laundromat',\n",
              " 'laura_lp',\n",
              " 'lazzzzzzzzzzzzzzzy',\n",
              " 'leapor',\n",
              " 'lemongrass',\n",
              " 'lena',\n",
              " 'life-story',\n",
              " 'lifechange',\n",
              " 'lightscribe',\n",
              " 'liking',\n",
              " 'linen',\n",
              " 'loooool',\n",
              " 'losing',\n",
              " 'lovelytrinket',\n",
              " 'loveyoujonesy',\n",
              " 'low-key',\n",
              " 'lunchfast',\n",
              " 'luttrell',\n",
              " 'macca',\n",
              " 'mahaz',\n",
              " 'mahon',\n",
              " 'mait',\n",
              " 'malign',\n",
              " 'malik',\n",
              " 'mandarinmonday',\n",
              " 'margarita',\n",
              " 'marlene',\n",
              " 'matutunaw',\n",
              " 'mclachlan',\n",
              " 'me',\n",
              " 'mediate',\n",
              " 'meehan',\n",
              " 'men',\n",
              " 'merton',\n",
              " 'met',\n",
              " 'metaprogramming',\n",
              " 'mi5',\n",
              " 'micheu',\n",
              " 'mileycyrus',\n",
              " 'minded',\n",
              " 'mins',\n",
              " 'misspelt',\n",
              " 'misspronounce',\n",
              " 'ml',\n",
              " 'monzer',\n",
              " 'moonwolf',\n",
              " 'morgannnn',\n",
              " 'mpr',\n",
              " 'mpumalanga',\n",
              " 'mtvaward',\n",
              " 'muahahahhahaha',\n",
              " 'mudweight',\n",
              " 'muir',\n",
              " 'mumford',\n",
              " 'mushygushy',\n",
              " 'muskie',\n",
              " 'mutate',\n",
              " 'mypowerbookg4rip',\n",
              " 'myself',\n",
              " 'nagustuhan',\n",
              " 'nasa',\n",
              " 'natalia',\n",
              " 'natasha',\n",
              " 'netflix',\n",
              " 'netsexor',\n",
              " 'neutral',\n",
              " 'nicolo',\n",
              " 'nigerianboi',\n",
              " 'nitty',\n",
              " 'nook',\n",
              " 'normand',\n",
              " 'notlooking',\n",
              " 'nsf',\n",
              " 'nylon',\n",
              " 'oestrus',\n",
              " 'ofcmate',\n",
              " 'officialshew',\n",
              " 'okapi',\n",
              " 'omginorite',\n",
              " 'omgssh',\n",
              " 'onigiri',\n",
              " 'oooommmmggg',\n",
              " 'oowww',\n",
              " 'openbox',\n",
              " 'oucccccccch',\n",
              " 'our',\n",
              " 'oxymoron',\n",
              " 'p.5',\n",
              " 'p11',\n",
              " 'pancho',\n",
              " 'paolo',\n",
              " 'papaver',\n",
              " 'parish',\n",
              " 'part-way',\n",
              " 'patti',\n",
              " 'payingitforward',\n",
              " 'penileword',\n",
              " 'pennywise',\n",
              " 'perkins',\n",
              " 'pesci',\n",
              " 'phootbooth',\n",
              " 'photofollow',\n",
              " 'photosensitivity',\n",
              " 'piggystuff',\n",
              " 'pinball',\n",
              " 'pinking',\n",
              " 'pittville',\n",
              " 'pleassseee',\n",
              " 'pleeaaasee',\n",
              " 'pollock',\n",
              " 'poorpoor',\n",
              " 'powersquid',\n",
              " 'prefers',\n",
              " 'prettifide',\n",
              " 'propellerhead',\n",
              " 'punkrockchick25',\n",
              " 'puny',\n",
              " 'pushing',\n",
              " 'quaker',\n",
              " 'raaaaaaaaaaagh',\n",
              " 'raaaain',\n",
              " 'rags',\n",
              " 'raspberries',\n",
              " 'reconstituted',\n",
              " 'reefer',\n",
              " 'remarry',\n",
              " 'resisting',\n",
              " 'resound',\n",
              " 'robben',\n",
              " 'rocksteady',\n",
              " 'rod',\n",
              " 'rosen',\n",
              " 'roskill',\n",
              " 'rostock',\n",
              " 'rot',\n",
              " 'rugged_man',\n",
              " 'sachin',\n",
              " 'sackville',\n",
              " 'saddddddd',\n",
              " 'saddened',\n",
              " 'said',\n",
              " 'salesperson',\n",
              " 'santos',\n",
              " 'sas',\n",
              " 'savings',\n",
              " 'scarepoint',\n",
              " 'scattegorie',\n",
              " 'schaumburg',\n",
              " 'screencast',\n",
              " 'sending',\n",
              " 'sent',\n",
              " 'serafina85',\n",
              " 'sexing',\n",
              " 'shankar',\n",
              " 'shannon1234',\n",
              " 'shaped',\n",
              " 'she',\n",
              " 'shittttttttt',\n",
              " 'shivshankar',\n",
              " 'shopoholic',\n",
              " 'short-range',\n",
              " 'showstoppe',\n",
              " 'shripriya',\n",
              " 'sicily',\n",
              " 'sidecar',\n",
              " 'sideshow',\n",
              " 'sidh',\n",
              " 'siiiiiickkk',\n",
              " 'simple_girl',\n",
              " 'sirius',\n",
              " 'sizzle',\n",
              " 'skimmed',\n",
              " 'skinnamarinkydinkydink',\n",
              " 'skyward',\n",
              " 'slamming',\n",
              " 'sleeeepe',\n",
              " 'sleeeepp',\n",
              " 'sleepnow',\n",
              " 'sleepypant',\n",
              " 'smee',\n",
              " 'snazzifie',\n",
              " 'snower',\n",
              " 'soberly',\n",
              " 'soccergame',\n",
              " 'softsynth',\n",
              " 'solh',\n",
              " 'songggggg',\n",
              " 'sorronda',\n",
              " 'spanikopita',\n",
              " 'spellings',\n",
              " 'sporadic',\n",
              " 'sporebat',\n",
              " 'squire',\n",
              " 'st.john',\n",
              " 'stabbing',\n",
              " 'stagnentation',\n",
              " 'stanage',\n",
              " 'stathamfans1',\n",
              " 'stellllaaa',\n",
              " 'stoooopid',\n",
              " 'stowey',\n",
              " 'stubhub',\n",
              " 'sucks',\n",
              " 'sucre',\n",
              " 'sukey',\n",
              " 'superbly',\n",
              " 'supercr3w',\n",
              " 'supposed',\n",
              " 'suuuppper',\n",
              " 'swirl',\n",
              " 'symbol',\n",
              " 'tangerine',\n",
              " 'tatars',\n",
              " 'tatiiiii',\n",
              " 'tea-tree',\n",
              " 'techmeme',\n",
              " 'tenant',\n",
              " 'testes',\n",
              " 'the_bigman',\n",
              " 'the_wockeez',\n",
              " 'they',\n",
              " 'thingsmummysaid',\n",
              " 'thnn',\n",
              " 'ticketmaster',\n",
              " 'tiiiire',\n",
              " 'time-zone',\n",
              " 'timeeee',\n",
              " 'timsamlake',\n",
              " 'tin',\n",
              " 'toblerone',\n",
              " 'todaaaay',\n",
              " 'top-hatted',\n",
              " 'touches',\n",
              " 'touvier',\n",
              " 'traceycake',\n",
              " 'trafficjam',\n",
              " 'transtelecom',\n",
              " 'traveller',\n",
              " 'treib',\n",
              " 'truetype',\n",
              " 'tumblarity',\n",
              " 'tweetboard',\n",
              " 'tweetdeck_0_25_manual_blink182',\n",
              " 'tweeteramas',\n",
              " 'tweeterize',\n",
              " 'tweetioi',\n",
              " 'twiddict',\n",
              " 'twitdroid',\n",
              " 'twitterererer',\n",
              " 'twitterlandz',\n",
              " 'twitterlove',\n",
              " 'twitterwar',\n",
              " 'tyrannosaurus',\n",
              " 'uncoooool',\n",
              " 'undaunted',\n",
              " 'undergone',\n",
              " 'underwired',\n",
              " 'unstuck',\n",
              " 'urghhhhhhhhhh',\n",
              " 'used',\n",
              " 'using',\n",
              " 'v6',\n",
              " 'videoblog',\n",
              " 'vivacity',\n",
              " 'w1p',\n",
              " 'waaalkiie',\n",
              " 'wahooooooo',\n",
              " 'walkathon',\n",
              " 'wally',\n",
              " 'warmfuzzie',\n",
              " 'warr',\n",
              " 'was',\n",
              " 'wavecut',\n",
              " 'we',\n",
              " 'weeks',\n",
              " 'wendyhouse',\n",
              " 'went',\n",
              " 'were',\n",
              " 'whangamata',\n",
              " 'whatevahh',\n",
              " 'wheeze',\n",
              " 'whooooooooooooa',\n",
              " 'whuuurrrrr',\n",
              " 'willed',\n",
              " 'winship',\n",
              " 'winterbone',\n",
              " 'women',\n",
              " 'wont',\n",
              " 'x.500',\n",
              " 'xiuxiu',\n",
              " 'xotashhh',\n",
              " 'y_spitta',\n",
              " 'yanto',\n",
              " 'yayayyayayay',\n",
              " 'yeeeewww',\n",
              " 'yellowcard',\n",
              " 'ymtumbkr',\n",
              " 'you',\n",
              " 'youremyheroine',\n",
              " 'yuuupp',\n",
              " 'yuuuuuum',\n",
              " 'zirconcode'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMyeK2j6jD-G",
        "colab_type": "text"
      },
      "source": [
        "More experiments when we get to build the model. Good to understand these information about the text tokens."
      ]
    }
  ]
}
