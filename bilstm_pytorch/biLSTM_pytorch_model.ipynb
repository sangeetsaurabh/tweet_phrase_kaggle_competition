{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "biLSTM_pytorch_model.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangeetsaurabh/tweet_sentiment_extraction/blob/master/bilstm_pytorch/biLSTM_pytorch_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5eDDoVRuCJh",
        "colab_type": "text"
      },
      "source": [
        "# 1 - BiLSTM model to predict selected text\n",
        "\n",
        "This is a bi-lstm PyTorch model that goes through each tweet and picks the phrase that should be selected.\n",
        "\n",
        "It's being implemented as a classification problem. So, for each word in the tweet model predicts if that word is selected or not.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y3MIJyd4IA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "db5e841d-ba1a-4c96-b3ee-561fd27bf7cc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dndOiMWUST2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = \"/content/drive/My Drive/tweet-sentiment-extraction/data\"\n",
        "tmp_folder = '/tmp'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE22B51XuCJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "import random\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGcjY5nCuCJl",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll set the random seeds for reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJptiXGJuCJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf4cyF7g_G7P",
        "colab_type": "text"
      },
      "source": [
        "#### Define the Spacy tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-YiuYTOMhmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_en = spacy.load('en')\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q8KEYcx_PXj",
        "colab_type": "text"
      },
      "source": [
        "##### Define the fields for PyTorch Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2yZpqaRQ2sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize = tokenize_en,\n",
        "                  init_token = '<sos>', \n",
        "                  eos_token = '<eos>',   \n",
        "                  lower = True) \n",
        "                  #include_lengths = True)\n",
        "SEL_TEXT = data.Field(tokenize = tokenize_en,\n",
        "                      init_token = '<sos>', \n",
        "                      eos_token = '<eos>',   \n",
        "                      lower = True,)\n",
        "LABEL = data.LabelField()\n",
        "IDX = data.Field(sequential=False,use_vocab=False,preprocessing=int)\n",
        "\n",
        "fields = [('text', TEXT),('sel_text', TEXT), ('idx', IDX), ('label', LABEL)]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-u6PspMFiWq",
        "colab_type": "text"
      },
      "source": [
        "Using Tabular dataset to read CSV files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_pFxNLLQ2sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset, valid_dataset, test_dataset = data.TabularDataset.splits(\n",
        "                                        path = data_folder,\n",
        "                                        train = 'train_transform.csv',\n",
        "                                        validation = 'valid_transform.csv',\n",
        "                                        test = 'test_transform.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields,\n",
        "                                        skip_header = True\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2c-JPcyuCJ3",
        "colab_type": "text"
      },
      "source": [
        "We can check how many examples are in each section of the dataset by checking their length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TD5FdJItQ2st",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1029ff3f-1e97-4961-8462-4ff741392e9d"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation examples: {len(valid_dataset)}\")\n",
        "print(f\"Number of testing examples: {len(test_dataset)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 26106\n",
            "Number of validation examples: 1374\n",
            "Number of testing examples: 3534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIbur7FhuCJ7",
        "colab_type": "text"
      },
      "source": [
        "Let's print out an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w23H6ZiuCJ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "229a56fd-2444-4d3e-8a4e-49add5fa56f2"
      },
      "source": [
        "print(vars(train_dataset.examples[0]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['neutral', '-pron-', '`', 'would', 'have', 'respond', ',', 'if', '-pron-', 'be', 'go', 'neutral'], 'sel_text': ['-pron-', '`', 'would', 'have', 'respond', ',', 'if', '-pron-', 'be', 'go'], 'idx': 0, 'label': 'neutral'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw0yMRTFuCJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "44a4e29a-6765-4db1-a584-b7aba11529d8"
      },
      "source": [
        "print(vars(train_dataset.examples[5]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['neutral', 'url', '-', 'some', 'shameless', 'plug', 'for', 'the', 'best', 'ranger', 'forum', 'on', 'earth', 'neutral'], 'sel_text': ['url', '-', 'some', 'shameless', 'plug', 'for', 'the', 'best', 'ranger', 'forum', 'on', 'earth'], 'idx': 5, 'label': 'neutral'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ptRQ-DluCKG",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll build the vocabulary - a mapping of tokens to integers. \n",
        "\n",
        "\n",
        "We also load the [GloVe](https://nlp.stanford.edu/projects/glove/) pre-trained token embeddings. Specifically, the 100-dimensional embeddings that have been trained on 6 billion tokens. Using pre-trained embeddings usually leads to improved performance - although admittedly the dataset used in this tutorial is too small to take advantage of the pre-trained embeddings. \n",
        "\n",
        "`unk_init` is used to initialize the token embeddings which are not in the pre-trained embedding vocabulary. By default this sets those embeddings to zeros, however it is better to not have them all initialized to the same value, so we initialize them from a Normal/Gaussian distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiufP1usuCKH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fb559b1d-8d18-4b6c-afb7-2a9e89216dbc"
      },
      "source": [
        "MIN_FREQ = 1\n",
        "\n",
        "TEXT.build_vocab(train_dataset, \n",
        "                 min_freq = MIN_FREQ,\n",
        "                 vectors = \"glove.6B.100d\",\n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_dataset)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                          \n",
            " 99%|█████████▉| 397720/400000 [00:15<00:00, 25046.61it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7hrhLTyuCKJ",
        "colab_type": "text"
      },
      "source": [
        "We can check how many tokens and tags are in our vocabulary by getting their length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F29RirhJuCKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81d4f2d6-280a-4557-e7e6-5a08e145c044"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 21271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbIp059uuCKL",
        "colab_type": "text"
      },
      "source": [
        "Exploring the vocabulary, we can check the most common tokens within our texts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjStS8mGuCKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1d3b3cbb-5f4f-443c-e379-f0831c34d2d1"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('-pron-', 62876), ('to', 24512), ('!', 22220), ('.', 21422), ('neutral', 21080), ('be', 19462), ('`', 17090), ('positive', 16409), ('negative', 14763), ('the', 12957), (',', 12334), ('going', 9845), ('*', 7350), ('and', 7152), ('have', 7008), ('?', 6316), ('s', 5803), ('in', 5540), ('...', 5535), ('for', 5244)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4IAf_eeuCKf",
        "colab_type": "text"
      },
      "source": [
        "The final part of data preparation is handling the iterator. \n",
        "\n",
        "This will be iterated over to return batches of data to process. Here, we set the batch size and the `device` - which is used to place the batches of tensors on our GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzIw-medVDPv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee39a7d2-2aaf-402f-f09a-03045ca9d4d6"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "#BATCH_SIZE = 4\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset, test_dataset), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x : len(x.text),\n",
        "     device = device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yndjUduEy0Me",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20cfecab-4a72-413a-b436-2756fc450aec"
      },
      "source": [
        "TEXT.vocab.itos[0:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', '<sos>', '<eos>', '-pron-', 'to', '!', '.', 'neutral', 'be']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov5l3XB-y6Gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unk_token_idx = TEXT.vocab.stoi['<unk>']\n",
        "pad_token_idx = TEXT.vocab.stoi['<pad>']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DEOwLgPGndr",
        "colab_type": "text"
      },
      "source": [
        "#### Output label preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bYFy0n_xALD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to go through each input tweet and return 0, 1, 2, or 3\n",
        "# 0 - word is not a selected phrase\n",
        "# 1 - word is a selected phrase\n",
        "# 2 - pad token\n",
        "# 3- unknown\n",
        "\n",
        "def find_subset_index(haystack, needle): \n",
        "    output = [0,0]\n",
        "    for idx, item in enumerate(haystack[2:len(haystack)-2]):\n",
        "        if item in [unk_token_idx]:\n",
        "          output.append(3)\n",
        "        elif item in [pad_token_idx]:\n",
        "          output.append(2)\n",
        "        elif item in needle:\n",
        "          output.append(1)\n",
        "        else:\n",
        "          output.append(0)\n",
        "    output = output + [0,0]\n",
        "    return output"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWu0C0Y4tHRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to prepare outout for a batch\n",
        "def return_output(src, trg):\n",
        "    output = []\n",
        "    output_flag = []\n",
        "    src = src.permute(1,0)\n",
        "    trg = trg.permute(1,0)\n",
        "    for bs in range(src.shape[0]):\n",
        "        trg_sentence = trg[bs,:].cpu().detach().numpy()\n",
        "        src_sentence = src[bs,:].cpu().detach().numpy()\n",
        "        #print(trg_sentence)\n",
        "        #print(src_sentence)\n",
        "       \n",
        "        \n",
        "        trg_index = find_subset_index(src_sentence, trg_sentence)\n",
        "        #print(trg_index)\n",
        "        #print(\"\\n\")\n",
        "        #print(trg_index)\n",
        "        if trg_index is None:\n",
        "          output_flag.append(batch_idx[bs].item())\n",
        "        output.append(trg_index)\n",
        "    output = torch.tensor(output).permute(1,0)\n",
        "    return output\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGWtRVQAYYcf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5aeefe27-edac-4453-8bc2-0b6a1e97e1b4"
      },
      "source": [
        "for i, batch in enumerate(valid_iterator):\n",
        "  \n",
        "  src = batch.text\n",
        "  trg = batch.sel_text\n",
        "  idx = batch.idx\n",
        "  \n",
        "\n",
        "  if i > 10:\n",
        "    \n",
        "    print(trg.permute(1,0))\n",
        "    print(\"\\n\")\n",
        "    x = return_output(src, trg)\n",
        "    print(src.permute(1,0))\n",
        "    print(\"\\n\")\n",
        "    print(x.permute(1,0))\n",
        "    print(idx)\n",
        "    break"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[    2,     4,   196,     9,   207,     3,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,     4,  9972,     3,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,   775,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,    36, 19014,    38,    42,     7,     4,    80,    63,   340,\n",
            "             7,     3],\n",
            "        [    2,  4814,    37,   291,   179,  1567,    14,     0,    17,     0,\n",
            "             7,     3],\n",
            "        [    2,    44,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,   907,    26,   952,     6,   952,     6,   952,     6,   225,\n",
            "            47,     3],\n",
            "        [    2,    60,    23,    13,  3047,     7,     3,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,  1384,   171,    22,     3,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,     4,  1013,     9,    91,     7,     3,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,   229,   171,   169,    15,     5,   125,   439,    33,     7,\n",
            "             3,     1],\n",
            "        [    2,   763,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,   741,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,    31,   242,    14,     4,    18,    15,     5,   391,     7,\n",
            "             3,     1],\n",
            "        [    2,     4,    44,  3321, 12357,    31,   103,    17,   263,  6817,\n",
            "             3,     1],\n",
            "        [    2,  1109,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,    49,   352,    49,  1582,     6,   545,    58,   177,   114,\n",
            "             3,     1],\n",
            "        [    2,     4,    79,   381,  7433,     3,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,    54,   967,   254,     4,    79,   137,  7839,   120,     6,\n",
            "             3,     1],\n",
            "        [    2,   405,    33,    18,     9,   255,    55,    55,     0,     7,\n",
            "             3,     1],\n",
            "        [    2,     4,   152,     9,   181,     7,     3,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,  2011,   635,   908,    13,  1802,  8048,     6,     6,     6,\n",
            "             3,     1],\n",
            "        [    2,    85,     4,    30,    14,     4,    82,   100,   325,     7,\n",
            "             3,     1],\n",
            "        [    2,   347,     7,     4,    89,     4,   101,    55,   177,     7,\n",
            "             3,     1],\n",
            "        [    2,    44,     3,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,   125,   241,    14,     3,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2, 13868,    89,    26,    27,    55,   177,     7,     3,     1,\n",
            "             1,     1],\n",
            "        [    2,     4,    44,     4,     6,     3,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,     4,    70,     4,     3,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    2,    53,    87,    10,    20,  3865,     7,    44,     4,     0,\n",
            "             3,     1],\n",
            "        [    2,     9,   171,    85,   708,    16,    16,    16,    16,   210,\n",
            "             3,     1],\n",
            "        [    2,     4,    70,     4,   677,     3,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "\n",
            "\n",
            "tensor([[    2,    12,     4,   170,   513,    77,   701,   102,     4,   196,\n",
            "             9,   207,    12,     3],\n",
            "        [    2,    11,     4,    58,     7,     4,  9972,     4,   375,    81,\n",
            "             4,     7,    11,     3],\n",
            "        [    2,    11,    21,  4306,    95,    25,    84,   192,     5,   775,\n",
            "             4,    47,    11,     3],\n",
            "        [    2,     8,    36, 19014,    38,    42,     7,     4,    80,    63,\n",
            "           340,     7,     8,     3],\n",
            "        [    2,     8,  4814,    37,   291,   179,  1567,    14,     0,    17,\n",
            "             0,     7,     8,     3],\n",
            "        [    2,    11,   541,     4,    66,   133,     6,     4,    44,    26,\n",
            "           241,     6,    11,     3],\n",
            "        [    2,    12,   907,    26,   952,     6,   952,     6,   952,     6,\n",
            "           225,    47,    12,     3],\n",
            "        [    2,    11,    60,    23,    13,  3047,     7,     0,   197,    14,\n",
            "             0,  3345,    11,     3],\n",
            "        [    2,    12,     9,  1384,   171,    22,    67,     5,  3229,    77,\n",
            "           258,   155,    12,     3],\n",
            "        [    2,    11,  4905,    18,   158,     0,     7,     4,  1013,     9,\n",
            "            91,     7,    11,     3],\n",
            "        [    2,    11,   229,   171,   169,    15,     5,   125,   439,    33,\n",
            "             7,    11,     3,     1],\n",
            "        [    2,    11,   763,    62,    29,  1534,     9,    46,    90,  1784,\n",
            "            23,    11,     3,     1],\n",
            "        [    2,    11,   741,    26,  1646,   659,   320,     5,     9,   104,\n",
            "            45,    11,     3,     1],\n",
            "        [    2,    12,    31,   242,    14,     4,    18,    15,     5,   391,\n",
            "             7,    12,     3,     1],\n",
            "        [    2,    11,     4,    44,  3321, 12357,    31,   103,    17,   263,\n",
            "          6817,    11,     3,     1],\n",
            "        [    2,    12,    98,     5,   155,     6,     6,    96,  1109,    73,\n",
            "          1012,    12,     3,     1],\n",
            "        [    2,     8,    49,   352,    49,  1582,     6,   545,    58,   177,\n",
            "           114,     8,     3,     1],\n",
            "        [    2,    12,     4,    79,   381,  7433,    17,    77,    21,    13,\n",
            "           641,    12,     3,     1],\n",
            "        [    2,     8,    54,   967,   254,     4,    79,   137,  7839,   120,\n",
            "             6,     8,     3,     1],\n",
            "        [    2,    11,   405,    33,    18,     9,   255,    55,    55,     0,\n",
            "             7,    11,     3,     1],\n",
            "        [    2,    12,     4,   152,     9,   181,     7,    80,     4,  1424,\n",
            "            19,    12,     3,     1],\n",
            "        [    2,    11,  2011,   635,   908,    13,  1802,  8048,     6,     6,\n",
            "             6,    11,     3,     1],\n",
            "        [    2,     8,    85,     4,    30,    14,     4,    82,   100,   325,\n",
            "             7,     8,     3,     1],\n",
            "        [    2,    11,   347,     7,     4,    89,     4,   101,    55,   177,\n",
            "             7,    11,     3,     1],\n",
            "        [    2,    11,    34,    27,    88,     4,    44,     0,    17,  1966,\n",
            "          3631,    11,     3,     1],\n",
            "        [    2,    11,   125,   241,    14,   199,    30,     4,    93,    73,\n",
            "            19,    11,     3,     1],\n",
            "        [    2,    11,     0,    76, 13868,    89,    26,    27,    55,   177,\n",
            "             7,    11,     3,     1],\n",
            "        [    2,    11,     4,    44,     4,     6,     4,   167,     9,   223,\n",
            "             6,    11,     3,     1],\n",
            "        [    2,    12,     4,    70,     4,     6,     6,     6,     6,    19,\n",
            "            19,    12,     3,     1],\n",
            "        [    2,    11,    53,    87,    10,    20,  3865,     7,    44,     4,\n",
            "             0,    11,     3,     1],\n",
            "        [    2,    12,     9,   171,    85,   708,    16,    16,    16,    16,\n",
            "           210,    12,     3,     1],\n",
            "        [    2,    12,     4,    70,     4,   677,   552,     4,    55,    28,\n",
            "          1168,    12,     3,     1]], device='cuda:0')\n",
            "\n",
            "\n",
            "tensor([[0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 3, 0, 0, 3, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 3, 1, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 3, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0]])\n",
            "tensor([26936,  4455,  7071,  5397,   291, 22104,  9445,  8063, 18877,   409,\n",
            "         6388, 22644,  4912, 16963, 13942,  4510, 19101, 24394, 17476, 15265,\n",
            "        26410,  8123, 13839, 17461, 12854, 26157,  2451, 26144,  4627,  5735,\n",
            "        11307, 15348], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrEMF6yJuCKi",
        "colab_type": "text"
      },
      "source": [
        "## Building the Model\n",
        "\n",
        "\n",
        "`nn.Embedding` is an embedding layer and the input dimension should be the size of the input (text) vocabulary. We tell it what the index of the padding token is so it does not update the padding token's embedding entry.\n",
        "\n",
        "`nn.LSTM` is the LSTM. We apply dropout as regularization between the layers, if we are using more than one.\n",
        "\n",
        "`nn.Linear` defines the linear layer to make predictions using the LSTM outputs. We double the size of the input if we are using a bi-directional LSTM. The output dimensions should be the size of the tag vocabulary.\n",
        "\n",
        "We also define a dropout layer with `nn.Dropout`, which we use in the `forward` method to apply dropout to the embeddings and the outputs of the final layer of the LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBPXgRwsuCKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTMPOSTagger(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim, \n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim, \n",
        "                            num_layers = n_layers, \n",
        "                            bidirectional = bidirectional,\n",
        "                            dropout = dropout if n_layers > 1 else 0)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        #pass text through embedding layer\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pass embeddings into LSTM\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        \n",
        "        #outputs holds the backward and forward hidden states in the final layer\n",
        "        #hidden and cell are the backward and forward hidden and cell states at the final time-step\n",
        "        \n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #we use our outputs to make a prediction of what the tag should be\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        \n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        \n",
        "        return predictions"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXi98ufguCKl",
        "colab_type": "text"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "Next, we instantiate the model. We need to ensure the embedding dimensions matches that of the GloVe embeddings we loaded earlier.\n",
        "\n",
        "The rest of the hyperparmeters have been chosen as sensible defaults, though there may be a combination that performs better on this model and dataset.\n",
        "\n",
        "The input and output dimensions are taken directly from the lengths of the respective vocabularies. The padding index is obtained using the vocabulary and the `Field` of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xcHqPQ9uCKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 4\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = BiLSTMPOSTagger(INPUT_DIM, \n",
        "                        EMBEDDING_DIM, \n",
        "                        HIDDEN_DIM, \n",
        "                        OUTPUT_DIM, \n",
        "                        N_LAYERS, \n",
        "                        BIDIRECTIONAL, \n",
        "                        DROPOUT, \n",
        "                        PAD_IDX)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgBVYHp5uCKo",
        "colab_type": "text"
      },
      "source": [
        "We initialize the weights from a simple Normal distribution. Again, there may be a better initialization scheme for this model and dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h39wBDRKuCKo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e8892a5b-05ac-460b-8fe9-6a11febdff23"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTMPOSTagger(\n",
              "  (embedding): Embedding(21271, 100, padding_idx=1)\n",
              "  (lstm): LSTM(100, 128, num_layers=2, dropout=0.25, bidirectional=True)\n",
              "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQSs1HqPuCKq",
        "colab_type": "text"
      },
      "source": [
        "Next, a small function to tell us how many parameters are in our model. Useful for comparing different models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh1iH6dduCKr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "134d0faf-c99a-426b-a151-6f6f93cd4c27"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,758,912 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5R90BZYuCKt",
        "colab_type": "text"
      },
      "source": [
        "We'll now initialize our model's embedding layer with the pre-trained embedding values we loaded earlier.\n",
        "\n",
        "This is done by getting them from the vocab's `.vectors` attribute and then performing a `.copy` to overwrite the embedding layer's current weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLG7V89QuCKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b3dce505-7465-47ca-ddf8-9d399baaed9f"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([21271, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpC33i28uCKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "724b613b-2bde-4e01-d448-2ca88b60489a"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
              "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
              "        [ 0.4298,  0.8205, -1.4562,  ...,  1.4802,  0.2942,  1.3924],\n",
              "        ...,\n",
              "        [-0.2532, -0.3959, -0.5765,  ..., -0.2383,  0.6833,  0.7435],\n",
              "        [ 0.7439, -0.0903,  0.4638,  ..., -1.1529, -1.1695,  0.6998],\n",
              "        [ 0.5389,  1.8131,  1.3117,  ...,  1.2478,  0.8841, -0.9864]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c5K2Mn9uCKx",
        "colab_type": "text"
      },
      "source": [
        "It's common to initialize the embedding of the pad token to all zeros. This, along with setting the `padding_idx` in the model's embedding layer, means that the embedding should always output a tensor full of zeros when a pad token is input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2MNLI8muCKy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "46e99e02-694b-4145-d6d0-44b4639d45b1"
      },
      "source": [
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.4298,  0.8205, -1.4562,  ...,  1.4802,  0.2942,  1.3924],\n",
            "        ...,\n",
            "        [-0.2532, -0.3959, -0.5765,  ..., -0.2383,  0.6833,  0.7435],\n",
            "        [ 0.7439, -0.0903,  0.4638,  ..., -1.1529, -1.1695,  0.6998],\n",
            "        [ 0.5389,  1.8131,  1.3117,  ...,  1.2478,  0.8841, -0.9864]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YvZyWk3uCK0",
        "colab_type": "text"
      },
      "source": [
        "We then define our optimizer, used to update our parameters w.r.t. their gradients. We use Adam with the default learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAmjQe8LuCK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg-a6UJDuCK2",
        "colab_type": "text"
      },
      "source": [
        "Next, we define our loss function, cross-entropy loss.\n",
        "\n",
        "Even though we have no `<unk>` tokens within our tag vocab, we still have `<pad>` tokens. This is because all sentences within a batch need to be the same size. However, we don't want to calculate the loss when the target is a `<pad>` token as we aren't training our model to recognize padding tokens.\n",
        "\n",
        "We handle this by setting the `ignore_index` in our loss function to the index of the padding token in our tag vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEbJhVfjuCK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TAG_PAD_IDX = 2\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFPr_FXeuCK4",
        "colab_type": "text"
      },
      "source": [
        "We then place our model and loss function on our GPU, if we have one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Istct-JWuCK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ivc5W3iuCK6",
        "colab_type": "text"
      },
      "source": [
        "We will be using the loss value between our predicted and actual tags to train the network, but ideally we'd like a more interpretable way to see how well our model is doing - accuracy.\n",
        "\n",
        "The issue is that we don't want to calculate accuracy over the `<pad>` tokens as we aren't interested in predicting them.\n",
        "\n",
        "The function below only calculates accuracy over non-padded tokens. `non_pad_elements` is a tensor containing the indices of the non-pad tokens within an input batch. We then compare the predictions of those elements with the labels to get a count of how many predictions were correct. We then divide this by the number of non-pad elements to get our accuracy value over the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZY7GZJeuCK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y, tag_pad_idx):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
        "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "envfqTC5uCK9",
        "colab_type": "text"
      },
      "source": [
        "Next is the function that handles training our model.\n",
        "\n",
        "We first set the model to `train` mode to turn on dropout/batch-norm/etc. (if used). Then we iterate over our iterator, which returns a batch of examples. \n",
        "\n",
        "For each batch: \n",
        "- we zero the gradients over the parameters from the last gradient calculation\n",
        "- insert the batch of text into the model to get predictions\n",
        "- as PyTorch loss functions cannot handle 3-dimensional predictions we reshape our predictions\n",
        "- calculate the loss and accuracy between the predicted tags and actual tags\n",
        "- call `backward` to calculate the gradients of the parameters w.r.t. the loss\n",
        "- take an optimizer `step` to update the parameters\n",
        "- add to the running total of loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ilLDQThuCK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "    \n",
        "        text = batch.text\n",
        "        sel_text = batch.sel_text\n",
        "        tags = return_output(text, sel_text)\n",
        "\n",
        "        text, tags = text.to(device), tags.to(device)\n",
        "                \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        predictions = model(text)\n",
        "        \n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "        \n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.contiguous().view(-1)\n",
        "        \n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "        #tags = [sent len * batch size]\n",
        "        \n",
        "        loss = criterion(predictions, tags)\n",
        "                \n",
        "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_1c43pHuCK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            sel_text = batch.sel_text\n",
        "            tags = return_output(text, sel_text)\n",
        "\n",
        "            text, tags = text.to(device), tags.to(device)\n",
        "            \n",
        "            predictions = model(text)\n",
        "            \n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.contiguous().view(-1)\n",
        "            \n",
        "            loss = criterion(predictions, tags)\n",
        "            \n",
        "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhQUPqH_uCLB",
        "colab_type": "text"
      },
      "source": [
        "Next, we have a small function that tells us how long an epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvTpNtw0uCLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQJ5D7R2uCLE",
        "colab_type": "text"
      },
      "source": [
        "Finally, we train our model!\n",
        "\n",
        "After each epoch we check if our model has achieved the best validation loss so far. If it has then we save the parameters of this model and we will use these \"best\" parameters to calculate performance over our test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpbhWy9buCLF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "471f7e41-a783-4071-f9ab-3cb90a8989b1"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.318 | Train Acc: 84.21%\n",
            "\t Val. Loss: 0.486 |  Val. Acc: 85.18%\n",
            "Epoch: 02 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.271 | Train Acc: 87.04%\n",
            "\t Val. Loss: 0.493 |  Val. Acc: 85.57%\n",
            "Epoch: 03 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.257 | Train Acc: 87.70%\n",
            "\t Val. Loss: 0.490 |  Val. Acc: 85.89%\n",
            "Epoch: 04 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.245 | Train Acc: 88.19%\n",
            "\t Val. Loss: 0.500 |  Val. Acc: 85.81%\n",
            "Epoch: 05 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.232 | Train Acc: 88.87%\n",
            "\t Val. Loss: 0.521 |  Val. Acc: 85.32%\n",
            "Epoch: 06 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.216 | Train Acc: 89.68%\n",
            "\t Val. Loss: 0.541 |  Val. Acc: 85.78%\n",
            "Epoch: 07 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.201 | Train Acc: 90.56%\n",
            "\t Val. Loss: 0.544 |  Val. Acc: 85.19%\n",
            "Epoch: 08 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.188 | Train Acc: 91.35%\n",
            "\t Val. Loss: 0.582 |  Val. Acc: 84.32%\n",
            "Epoch: 09 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.175 | Train Acc: 91.97%\n",
            "\t Val. Loss: 0.602 |  Val. Acc: 84.54%\n",
            "Epoch: 10 | Epoch Time: 0m 12s\n",
            "\tTrain Loss: 0.162 | Train Acc: 92.69%\n",
            "\t Val. Loss: 0.652 |  Val. Acc: 84.51%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkUtfYbiuCLH",
        "colab_type": "text"
      },
      "source": [
        "We then load our \"best\" parameters and evaluate performance on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR2jrbHsuCLI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4684f1e-3d9f-4406-ae8a-64d27d6dcb61"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.486 |  Test Acc: 85.18%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMT9169vuCLK",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "\n",
        "85% accuracy looks pretty good, but let's see our model tag some actual sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iv24bT1uCLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Function to test against an actual tweet\n",
        "def tag_sentence(model, device, sentence, text_field):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    if isinstance(sentence, str):\n",
        "        nlp = spacy.load('en')\n",
        "        tokens = [token.text for token in nlp(sentence)]\n",
        "    else:\n",
        "        tokens = [token for token in sentence]\n",
        "\n",
        "    if text_field.lower:\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "        \n",
        "    numericalized_tokens = [text_field.vocab.stoi[t] for t in tokens]\n",
        "    #print(numericalized_tokens)\n",
        "    #numericalized_tokens = tokens\n",
        "\n",
        "    unk_idx = text_field.vocab.stoi[text_field.unk_token]\n",
        "    \n",
        "    unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n",
        "\n",
        "    numericalized_tokens = [text_field.vocab.stoi['<sos>']] + numericalized_tokens + [text_field.vocab.stoi[TEXT.eos_token]]\n",
        "    \n",
        "    token_tensor = torch.LongTensor(numericalized_tokens)\n",
        "    \n",
        "    token_tensor = token_tensor.unsqueeze(-1).to(device)\n",
        "         \n",
        "    predictions = model(token_tensor)\n",
        "    \n",
        "    top_predictions = predictions.argmax(-1)\n",
        "    \n",
        "    predicted_tags = top_predictions.cpu().detach().numpy()\n",
        "    \n",
        "    return numericalized_tokens, predicted_tags.squeeze(1), unks"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZjZ9hT6uCLO",
        "colab_type": "text"
      },
      "source": [
        "We'll get an already tokenized example from the training set and test our model's performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY0DIPdHuCLO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e02c1141-ed1e-490b-f984-511a2341884e"
      },
      "source": [
        "example_index = 1\n",
        "\n",
        "text = vars(valid_dataset.examples[example_index])['text']\n",
        "actual_tags = vars(valid_dataset.examples[example_index])['sel_text']\n",
        "\n",
        "\n",
        "print(text)\n",
        "print (actual_tags)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['negative', '_', 'wx', 'do', '-pron-', 'see', 'the', 'color', 'of', 'the', 'sky', 'and', 'how', '-pron-', 'look', 'in', 'philly', '?', '?', '-pron-', 'be', 'yellowish', '/', 'orangeish', '/', 'brownish', 'look', 'scary', '!', '!', 'lol', 'negative']\n",
            "['look', 'scary', '!', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky2UYB1CCa0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "35fdfdcc-a46d-4cde-884a-db51888626e5"
      },
      "source": [
        "example_index = 1\n",
        "text = vars(valid_dataset.examples[example_index])['text']\n",
        "sel_text = vars(valid_dataset.examples[example_index])['sel_text']\n",
        "idx = vars(valid_dataset.examples[example_index])['idx']\n",
        "\n",
        "sel_text_token = [TEXT.vocab.stoi[token] for token in sel_text]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(text)\n",
        "print (sel_text)\n",
        "print(idx)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['negative', '_', 'wx', 'do', '-pron-', 'see', 'the', 'color', 'of', 'the', 'sky', 'and', 'how', '-pron-', 'look', 'in', 'philly', '?', '?', '-pron-', 'be', 'yellowish', '/', 'orangeish', '/', 'brownish', 'look', 'scary', '!', '!', 'lol', 'negative']\n",
            "['look', 'scary', '!', '!']\n",
            "16071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VCsnzREGNbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "522730e9-dbd5-41db-ea47-0cecee0e2763"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "valid_data = pd.read_csv(data_folder + '/valid_transform.csv')\n",
        "valid_data[valid_data.idx == idx]"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>idx</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>negative _wx do -PRON- see the color of the sky and how -PRON- look in philly ?? -PRON- be yellowish / orangeish / brownish look scary !! lol negative</td>\n",
              "      <td>look scary !!</td>\n",
              "      <td>16071</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                     text  ... sentiment\n",
              "1  negative _wx do -PRON- see the color of the sky and how -PRON- look in philly ?? -PRON- be yellowish / orangeish / brownish look scary !! lol negative  ...  negative\n",
              "\n",
              "[1 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gV82rTIuCLR",
        "colab_type": "text"
      },
      "source": [
        "We can then use our `tag_sentence` function to get the tags. Notice how the tokens referring to subject of the sentence, the \"respected cleric\", are both `<unk>` tokens!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUHrtNIzuCLR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3118b562-26a4-4f98-fc58-f62cc037a1b7"
      },
      "source": [
        "tokens, pred_tags, unks = tag_sentence(model, \n",
        "                                       device, \n",
        "                                       text,\n",
        "                                       TEXT)\n",
        "\n",
        "actual_tags = find_subset_index(tokens,sel_text_token)\n",
        "\n",
        "print(tokens)\n",
        "print(actual_tags)\n",
        "print(pred_tags)\n",
        "print(unks)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 12, 54, 8117, 30, 4, 56, 13, 1625, 25, 13, 1209, 17, 92, 4, 124, 21, 2045, 19, 19, 4, 9, 0, 135, 0, 135, 0, 124, 957, 6, 6, 65, 12, 3]\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 3, 1, 1, 1, 1, 0, 0, 0]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
            "['yellowish', 'orangeish', 'brownish']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frEcOTSKuCLU",
        "colab_type": "text"
      },
      "source": [
        "We can then check how well it did. Surprisingly, it got every token correct, including the two that were unknown tokens!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GpeXs9wuCLU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "191beb02-1ba9-4b72-9b0e-446b110902a0"
      },
      "source": [
        "print(\"Pred. Tag\\tActual Tag\\tCorrect?\\tToken\\n\")\n",
        "\n",
        "for token, pred_tag, actual_tag in zip(tokens, pred_tags, actual_tags):\n",
        "    correct = '✔' if pred_tag == actual_tag else '✘'\n",
        "    print(f\"{pred_tag}\\t\\t{actual_tag}\\t\\t{correct}\\t\\t{TEXT.vocab.itos[token]}\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pred. Tag\tActual Tag\tCorrect?\tToken\n",
            "\n",
            "0\t\t0\t\t✔\t\t<sos>\n",
            "0\t\t0\t\t✔\t\tnegative\n",
            "0\t\t0\t\t✔\t\t_\n",
            "0\t\t0\t\t✔\t\twx\n",
            "0\t\t0\t\t✔\t\tdo\n",
            "0\t\t0\t\t✔\t\t-pron-\n",
            "0\t\t0\t\t✔\t\tsee\n",
            "0\t\t0\t\t✔\t\tthe\n",
            "0\t\t0\t\t✔\t\tcolor\n",
            "0\t\t0\t\t✔\t\tof\n",
            "0\t\t0\t\t✔\t\tthe\n",
            "0\t\t0\t\t✔\t\tsky\n",
            "0\t\t0\t\t✔\t\tand\n",
            "0\t\t0\t\t✔\t\thow\n",
            "0\t\t0\t\t✔\t\t-pron-\n",
            "0\t\t1\t\t✘\t\tlook\n",
            "0\t\t0\t\t✔\t\tin\n",
            "0\t\t0\t\t✔\t\tphilly\n",
            "0\t\t0\t\t✔\t\t?\n",
            "0\t\t0\t\t✔\t\t?\n",
            "0\t\t0\t\t✔\t\t-pron-\n",
            "0\t\t0\t\t✔\t\tbe\n",
            "0\t\t3\t\t✘\t\t<unk>\n",
            "0\t\t0\t\t✔\t\t/\n",
            "0\t\t3\t\t✘\t\t<unk>\n",
            "0\t\t0\t\t✔\t\t/\n",
            "0\t\t3\t\t✘\t\t<unk>\n",
            "0\t\t1\t\t✘\t\tlook\n",
            "1\t\t1\t\t✔\t\tscary\n",
            "1\t\t1\t\t✔\t\t!\n",
            "0\t\t1\t\t✘\t\t!\n",
            "0\t\t0\t\t✔\t\tlol\n",
            "0\t\t0\t\t✔\t\tnegative\n",
            "0\t\t0\t\t✔\t\t<eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQYTLxKscey",
        "colab_type": "text"
      },
      "source": [
        "Not bad. Almost gets it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKSMMmzTsgeE",
        "colab_type": "text"
      },
      "source": [
        "### Jaccard testing\n",
        "\n",
        "Since Kaggle submission will test against Jaccard, let's see how the model performs against Jaccard score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j24I1sK8tpnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1q9x8g3JsPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_df = []\n",
        "\n",
        "for i in range(0, len(valid_dataset)):\n",
        "\n",
        "\n",
        "  output_dict = {}\n",
        "  valid_idx = vars(valid_dataset.examples[i])['idx']\n",
        "  output_dict[\"idx\"] = valid_idx\n",
        "\n",
        "  src = vars(valid_dataset.examples[i])['text']\n",
        "  output_dict[\"text\"] = \" \".join(src)\n",
        "  #print(output_dict[\"text\"])\n",
        "\n",
        "  trg = vars(valid_dataset.examples[i])['sel_text']\n",
        "  output_dict[\"selected_text\"] = \" \".join(trg)\n",
        "  #print(output_dict[\"selected_text\"])\n",
        "\n",
        "  snt = vars(valid_dataset.examples[i])['label']\n",
        "  output_dict[\"sentiment\"] = snt\n",
        "\n",
        "  tokens, predicted_tags, unks = tag_sentence(model, \n",
        "                                       device, \n",
        "                                       src,\n",
        "                                       TEXT)\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "  pred_indx = [indx for indx, x in enumerate(predicted_tags[:-1]) if x == 1]\n",
        "  output_dict[\"predicted_text\"] = \" \".join([TEXT.vocab.itos[tokens[p]] for p in pred_indx])\n",
        "  #print(pred_indx)\n",
        "\n",
        "\n",
        "  output_dict[\"baseline_score\"] = jaccard(output_dict[\"text\"], output_dict[\"selected_text\"])\n",
        "  output_dict[\"j_score\"] = jaccard(output_dict[\"predicted_text\"], output_dict[\"selected_text\"])\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  valid_df.append(output_dict)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enVTkWIVaQht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "outputId": "b707b5e6-a938-4bba-f57f-0d1840c9e17a"
      },
      "source": [
        "valid_df = pd.DataFrame(valid_df)\n",
        "valid_df.tail(10)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>predicted_text</th>\n",
              "      <th>baseline_score</th>\n",
              "      <th>j_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1364</th>\n",
              "      <td>14443</td>\n",
              "      <td>neutral crawling back into bed ... because -pron- can neutral</td>\n",
              "      <td>crawling back into bed ... because -pron- can</td>\n",
              "      <td>neutral</td>\n",
              "      <td>crawling back into bed ... because -pron- can</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1365</th>\n",
              "      <td>14636</td>\n",
              "      <td>positive -pron- friend be awesome ! -- and the non twitt one here right now too ! ! positive</td>\n",
              "      <td>-pron- friend be awesome !</td>\n",
              "      <td>positive</td>\n",
              "      <td>awesome</td>\n",
              "      <td>0.312500</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1366</th>\n",
              "      <td>3329</td>\n",
              "      <td>negative - l ` would come if u could , but australia be just too far away negative</td>\n",
              "      <td>far</td>\n",
              "      <td>negative</td>\n",
              "      <td>too far away</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1367</th>\n",
              "      <td>23117</td>\n",
              "      <td>neutral just sittin here listenin to music . follow -pron- ? neutral</td>\n",
              "      <td>just sittin here listenin to music . follow -pron- ?</td>\n",
              "      <td>neutral</td>\n",
              "      <td>just sittin here listenin to music . follow -pron- ?</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1368</th>\n",
              "      <td>25902</td>\n",
              "      <td>positive infamous on the ps3 = awesome . -pron- eye be so sore now though positive</td>\n",
              "      <td>awesome .</td>\n",
              "      <td>positive</td>\n",
              "      <td>awesome .</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1369</th>\n",
              "      <td>7493</td>\n",
              "      <td>positive finally , -pron- get -pron- teaching load confusion clear . -pron- will teach 3 third year section but with going to catch . positive</td>\n",
              "      <td>finally , -pron- get -pron- teaching load confusion clear .</td>\n",
              "      <td>positive</td>\n",
              "      <td>. .</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.111111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1370</th>\n",
              "      <td>26744</td>\n",
              "      <td>neutral -pron- do the same thing in nola neutral</td>\n",
              "      <td>-pron- do the same thing in nola</td>\n",
              "      <td>neutral</td>\n",
              "      <td>-pron- do the same thing in nola</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1371</th>\n",
              "      <td>693</td>\n",
              "      <td>positive thank for the greeting positive</td>\n",
              "      <td>thank</td>\n",
              "      <td>positive</td>\n",
              "      <td>thank</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1372</th>\n",
              "      <td>22833</td>\n",
              "      <td>neutral eating maccie neutral</td>\n",
              "      <td>eating maccie</td>\n",
              "      <td>neutral</td>\n",
              "      <td>eating &lt;unk&gt;</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1373</th>\n",
              "      <td>22906</td>\n",
              "      <td>neutral do with the packing and everything else ... leaving in 3 hour ... neutral</td>\n",
              "      <td>do with the packing and everything else ... leaving in 3 hour ...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>do with the packing and everything else ... leaving in 3 hour ...</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        idx  ...   j_score\n",
              "1364  14443  ...  1.000000\n",
              "1365  14636  ...  0.200000\n",
              "1366   3329  ...  0.333333\n",
              "1367  23117  ...  1.000000\n",
              "1368  25902  ...  1.000000\n",
              "1369   7493  ...  0.111111\n",
              "1370  26744  ...  1.000000\n",
              "1371    693  ...  1.000000\n",
              "1372  22833  ...  0.333333\n",
              "1373  22906  ...  1.000000\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEfCsUopbRXE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e77d56e-566d-43a7-adb2-6d6c57483173"
      },
      "source": [
        "round(valid_df.j_score.mean(),2) ### j-score using algorithm"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.61"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzxxG7XSbW5w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "46685bae-2ac4-414b-f730-7adc8bb4371e"
      },
      "source": [
        "round(valid_df.baseline_score.mean(),2) ### j-score using baseline"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.56"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6EC4jV7tRGJ",
        "colab_type": "text"
      },
      "source": [
        "#### Conclusion\n",
        "\n",
        "The model performs better than the baseline. But, it needs to become much better to compete on the leaderboard"
      ]
    }
  ]
}