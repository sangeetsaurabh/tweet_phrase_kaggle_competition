{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "keras_roberta_implementation.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05d4554ca7b8426d8f82dab40e1d7a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f5ffecae448646d09757d2680c39689a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_38ff005630a042da9dcd56f85b3f6aa1",
              "IPY_MODEL_52bac21e2a7d4e7483d408cc04db18d4"
            ]
          }
        },
        "f5ffecae448646d09757d2680c39689a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38ff005630a042da9dcd56f85b3f6aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ac6559748bf840bb87c742f83e2a318c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 482,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 482,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee4c1077ca564b7594576ac3f3c8d053"
          }
        },
        "52bac21e2a7d4e7483d408cc04db18d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9e9c5f3fd4274d6a904d767a432c049e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 482/482 [01:03&lt;00:00, 7.64B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_28bbdb2bafcc455289cd04d2896a34ca"
          }
        },
        "ac6559748bf840bb87c742f83e2a318c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee4c1077ca564b7594576ac3f3c8d053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e9c5f3fd4274d6a904d767a432c049e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "28bbdb2bafcc455289cd04d2896a34ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83980c2aa22f45e9b2e8b868a3df2c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_08f978247e264abe8c2da39ef8c245a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_be48affa77bc475d90739fd69abfcca5",
              "IPY_MODEL_6eca639e2f3c45b091b6fbe5ad3b4d4f"
            ]
          }
        },
        "08f978247e264abe8c2da39ef8c245a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be48affa77bc475d90739fd69abfcca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f38ab976ad4d434b9d34e3e4ab97f310",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1634375628,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1634375628,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_81e91c1af9904034973c42169d35ea5a"
          }
        },
        "6eca639e2f3c45b091b6fbe5ad3b4d4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e93ff930fa1f40a0bfaa08ebc0725ed0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.63G/1.63G [01:02&lt;00:00, 26.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b17f6ca0b9134ab5b369f614d6ff1149"
          }
        },
        "f38ab976ad4d434b9d34e3e4ab97f310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "81e91c1af9904034973c42169d35ea5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e93ff930fa1f40a0bfaa08ebc0725ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b17f6ca0b9134ab5b369f614d6ff1149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sangeetsaurabh/tweet_sentiment_extraction/blob/master/keras_transformer/keras_roberta_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mZrQJOsdUjh",
        "colab_type": "text"
      },
      "source": [
        "# Keras Roberta implementation\n",
        "\n",
        "This file is implementing Roberta transformer in Keras to predict selected text for Kaggle competition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y3MIJyd4IA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "bc1006fe-584b-472a-b1c0-6bf1e2aa5852"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dndOiMWUST2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = \"/content/drive/My Drive/tweet-sentiment-extraction/data\"\n",
        "roberta_folder = \"/content/drive/My Drive/tweet-sentiment-extraction/roberta/\"\n",
        "tmp_folder = '/tmp'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXo7qOUs6H9d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d80ffe6e-a65d-4f1a-9920-a1ce8e643b75"
      },
      "source": [
        "import shutil\n",
        "shutil.copyfile('/content/drive/My Drive/tweet-sentiment-extraction/data/roberta-large-merges.txt', '/tmp/roberta-large-merges.txt')\n",
        "shutil.copyfile('/content/drive/My Drive/tweet-sentiment-extraction/data/roberta-large-vocab.json', '/tmp/roberta-large-vocab.json')\n",
        "shutil.copyfile('/content/drive/My Drive/tweet-sentiment-extraction/data/roberta-base-config.json', '/tmp/roberta-base-config.json')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/tmp/roberta-base-config.json'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dTtyM3EdLZE",
        "colab_type": "text"
      },
      "source": [
        "#### Install the right modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YCGplAqdLZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "cb9975ba-42da-41ec-8048-63b310b5b239"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
            "\r\u001b[K     |▍                               | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 81kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 757kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 17.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 20.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 25.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=b399b6cde2f97b936327a9d2b146e088b1a85541b76b8301095f2e7a243d2143\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I0vrSGxdLZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28edce68-da5f-434e-dbc6-12eded5ba07d"
      },
      "source": [
        "import pandas as pd, numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "import math\n",
        "print('TF version',tf.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version 2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDbm8U7adLZQ",
        "colab_type": "text"
      },
      "source": [
        "### Prepare the input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7xhnCGHdLZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "fcfb86ab-78b7-4e8b-e2b5-74fe0dbdefed"
      },
      "source": [
        "MAX_LEN = 96\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=tmp_folder+'/roberta-large-vocab.json', \n",
        "    merges_file=tmp_folder+'/roberta-large-merges.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "EPOCHS = 3 # originally 3\n",
        "BATCH_SIZE = 32 # originally 32\n",
        "PAD_ID = 1\n",
        "SEED = 88888\n",
        "LABEL_SMOOTHING = 0.1\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
        "train = pd.read_csv(data_folder + '/train.csv').fillna('')\n",
        "train.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>selected_text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>cb774db0d1</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>I`d have responded, if I were going</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>549e992a42</td>\n",
              "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
              "      <td>Sooo SAD</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>088c60f138</td>\n",
              "      <td>my boss is bullying me...</td>\n",
              "      <td>bullying me</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9642c003ef</td>\n",
              "      <td>what interview! leave me alone</td>\n",
              "      <td>leave me alone</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>358bd9e861</td>\n",
              "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
              "      <td>Sons of ****,</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       textID  ... sentiment\n",
              "0  cb774db0d1  ...   neutral\n",
              "1  549e992a42  ...  negative\n",
              "2  088c60f138  ...  negative\n",
              "3  9642c003ef  ...  negative\n",
              "4  358bd9e861  ...  negative\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpCqbtnDdLZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32ff4be6-c05e-489a-aa87-a8d58b2dfcb0"
      },
      "source": [
        "tokenizer.get_vocab_size()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50265"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZjD5a_rdLZX",
        "colab_type": "text"
      },
      "source": [
        "#### Convert to training data based upon Roberta input requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXyNP04KdLZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = train.shape[0]\n",
        "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(train.shape[0]):\n",
        "    \n",
        "    # FIND OVERLAP\n",
        "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
        "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
        "    idx = text1.find(text2)\n",
        "    chars = np.zeros((len(text1)))\n",
        "    chars[idx:idx+len(text2)]=1\n",
        "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "    enc = tokenizer.encode(text1) \n",
        "        \n",
        "    # ID_OFFSETS\n",
        "    offsets = []; idx=0\n",
        "    for t in enc.ids:\n",
        "        w = tokenizer.decode([t])\n",
        "        offsets.append((idx,idx+len(w)))\n",
        "        idx += len(w)\n",
        "    \n",
        "    # START END TOKENS\n",
        "    toks = []\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.sum(chars[a:b])\n",
        "        if sm>0: toks.append(i) \n",
        "        \n",
        "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
        "    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
        "    attention_mask[k,:len(enc.ids)+3] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+2] = 1\n",
        "        end_tokens[k,toks[-1]+2] = 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLU5IrTYdLZZ",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare the test data as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB_GvJkIdLZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv(data_folder + '/test.csv').fillna('')\n",
        "\n",
        "ct = test.shape[0]\n",
        "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(test.shape[0]):\n",
        "        \n",
        "    # INPUT_IDS\n",
        "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "    enc = tokenizer.encode(text1)                \n",
        "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
        "    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
        "    attention_mask_t[k,:len(enc.ids)+3] = 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXk5baUydLZc",
        "colab_type": "text"
      },
      "source": [
        "#### Build Roberta Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOmh8cLRdLZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "def save_weights(model, dst_fn):\n",
        "    weights = model.get_weights()\n",
        "    with open(dst_fn, 'wb') as f:\n",
        "        pickle.dump(weights, f)\n",
        "\n",
        "\n",
        "def load_weights(model, weight_fn):\n",
        "    with open(weight_fn, 'rb') as f:\n",
        "        weights = pickle.load(f)\n",
        "    model.set_weights(weights)\n",
        "    return model\n",
        "\n",
        "def loss_fn(y_true, y_pred):\n",
        "    # adjust the targets for sequence bucketing\n",
        "    ll = tf.shape(y_pred)[1]\n",
        "    y_true = y_true[:, :ll]\n",
        "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
        "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
        "\n",
        "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
        "    max_len = tf.reduce_max(lens)\n",
        "    ids_ = ids[:, :max_len]\n",
        "    att_ = att[:, :max_len]\n",
        "    tok_ = tok[:, :max_len]\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(\"roberta-large\")\n",
        "    bert_model = TFRobertaModel.from_pretrained(\"roberta-large\",config=config)\n",
        "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
        "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n",
        "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
        "    \n",
        "    # this is required as `model.predict` needs a fixed size!\n",
        "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
        "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
        "    \n",
        "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
        "    return model, padded_model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koRQHTeOdLZf",
        "colab_type": "text"
      },
      "source": [
        "#### Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztCvkNpUdLZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    if (len(a)==0) & (len(b)==0): return 0.5\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIuMutQwdLZj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f38db224-cb8e-451d-bb5d-b8e4e66e801a"
      },
      "source": [
        "test.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3534, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGsF97RUdLZl",
        "colab_type": "text"
      },
      "source": [
        "#### Train the Roberta model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPDX94tAdLZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "05d4554ca7b8426d8f82dab40e1d7a6b",
            "f5ffecae448646d09757d2680c39689a",
            "38ff005630a042da9dcd56f85b3f6aa1",
            "52bac21e2a7d4e7483d408cc04db18d4",
            "ac6559748bf840bb87c742f83e2a318c",
            "ee4c1077ca564b7594576ac3f3c8d053",
            "9e9c5f3fd4274d6a904d767a432c049e",
            "28bbdb2bafcc455289cd04d2896a34ca",
            "83980c2aa22f45e9b2e8b868a3df2c1a",
            "08f978247e264abe8c2da39ef8c245a6",
            "be48affa77bc475d90739fd69abfcca5",
            "6eca639e2f3c45b091b6fbe5ad3b4d4f",
            "f38ab976ad4d434b9d34e3e4ab97f310",
            "81e91c1af9904034973c42169d35ea5a",
            "e93ff930fa1f40a0bfaa08ebc0725ed0",
            "b17f6ca0b9134ab5b369f614d6ff1149"
          ]
        },
        "outputId": "e6c93915-5ec1-4e4b-aefd-8203718a39eb"
      },
      "source": [
        "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=SEED) #originally 5 splits\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print('### FOLD %i'%(fold+1))\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model, padded_model = build_model()\n",
        "        \n",
        "    #sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "    #    save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n",
        "    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n",
        "    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n",
        "    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n",
        "    # sort the validation data\n",
        "    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n",
        "    inpV = [arr[shuffleV] for arr in inpV]\n",
        "    targetV = [arr[shuffleV] for arr in targetV]\n",
        "    #weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n",
        "    weight_fn = 'roberta.h5'\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        # sort and shuffle: We add random numbers to not have the same order in each epoch\n",
        "        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n",
        "        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n",
        "        num_batches = math.ceil(len(shuffleT) / BATCH_SIZE)\n",
        "        batch_inds = np.random.permutation(num_batches)\n",
        "        shuffleT_ = []\n",
        "        for batch_ind in batch_inds:\n",
        "            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n",
        "        shuffleT = np.concatenate(shuffleT_)\n",
        "        # reorder the input data\n",
        "        inpT = [arr[shuffleT] for arr in inpT]\n",
        "        targetT = [arr[shuffleT] for arr in targetT]\n",
        "        model.fit(inpT, targetT, \n",
        "            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n",
        "            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n",
        "        save_weights(model, weight_fn)\n",
        "\n",
        "    print('Loading model...')\n",
        "    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "    load_weights(model, weight_fn)\n",
        "\n",
        "    print('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    print('Predicting Test...')\n",
        "    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "    preds_start += preds[0]/skf.n_splits\n",
        "    preds_end += preds[1]/skf.n_splits\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-2:b-1])\n",
        "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
        "    print()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#########################\n",
            "### FOLD 1\n",
            "#########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05d4554ca7b8426d8f82dab40e1d7a6b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=482.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83980c2aa22f45e9b2e8b868a3df2c1a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1634375628.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-large.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "687/687 [==============================] - 257s 374ms/step - loss: 2.9305 - activation_loss: 1.4751 - activation_1_loss: 1.4554 - val_loss: 2.5125 - val_activation_loss: 1.2711 - val_activation_1_loss: 1.2415\n",
            "Epoch 2/2\n",
            "687/687 [==============================] - 250s 364ms/step - loss: 2.5381 - activation_loss: 1.2897 - activation_1_loss: 1.2484 - val_loss: 2.5368 - val_activation_loss: 1.3004 - val_activation_1_loss: 1.2363\n",
            "Epoch 3/3\n",
            "687/687 [==============================] - 250s 364ms/step - loss: 2.4315 - activation_loss: 1.2313 - activation_1_loss: 1.2002 - val_loss: 2.5196 - val_activation_loss: 1.2905 - val_activation_1_loss: 1.2291\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 32s 186ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 183ms/step\n",
            ">>>> FOLD 1 Jaccard = 0.7073552574291998\n",
            "\n",
            "#########################\n",
            "### FOLD 2\n",
            "#########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-large.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - 257s 374ms/step - loss: 3.0893 - activation_loss: 1.5588 - activation_1_loss: 1.5305 - val_loss: 2.5726 - val_activation_loss: 1.3189 - val_activation_1_loss: 1.2537\n",
            "Epoch 2/2\n",
            "688/688 [==============================] - 254s 369ms/step - loss: 2.5971 - activation_loss: 1.3165 - activation_1_loss: 1.2805 - val_loss: 2.5292 - val_activation_loss: 1.3090 - val_activation_1_loss: 1.2202\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - 270s 392ms/step - loss: 2.5402 - activation_loss: 1.2929 - activation_1_loss: 1.2472 - val_loss: 2.5833 - val_activation_loss: 1.3098 - val_activation_1_loss: 1.2735\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 32s 184ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 183ms/step\n",
            ">>>> FOLD 2 Jaccard = 0.7048464759687185\n",
            "\n",
            "#########################\n",
            "### FOLD 3\n",
            "#########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-large.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - 262s 381ms/step - loss: 3.2386 - activation_loss: 1.6328 - activation_1_loss: 1.6058 - val_loss: 2.7409 - val_activation_loss: 1.3591 - val_activation_1_loss: 1.3818\n",
            "Epoch 2/2\n",
            "688/688 [==============================] - 252s 367ms/step - loss: 2.6221 - activation_loss: 1.3294 - activation_1_loss: 1.2927 - val_loss: 2.5729 - val_activation_loss: 1.3011 - val_activation_1_loss: 1.2718\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - 259s 377ms/step - loss: 2.5097 - activation_loss: 1.2748 - activation_1_loss: 1.2349 - val_loss: 2.5125 - val_activation_loss: 1.2724 - val_activation_1_loss: 1.2401\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 31s 183ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 183ms/step\n",
            ">>>> FOLD 3 Jaccard = 0.7041639892302929\n",
            "\n",
            "#########################\n",
            "### FOLD 4\n",
            "#########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-large.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - 259s 377ms/step - loss: 3.2621 - activation_loss: 1.6638 - activation_1_loss: 1.5982 - val_loss: 2.6654 - val_activation_loss: 1.3572 - val_activation_1_loss: 1.3082\n",
            "Epoch 2/2\n",
            "688/688 [==============================] - 253s 367ms/step - loss: 2.6200 - activation_loss: 1.3235 - activation_1_loss: 1.2965 - val_loss: 2.6080 - val_activation_loss: 1.3282 - val_activation_1_loss: 1.2798\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - 280s 406ms/step - loss: 2.6121 - activation_loss: 1.3192 - activation_1_loss: 1.2930 - val_loss: 2.5586 - val_activation_loss: 1.3030 - val_activation_1_loss: 1.2556\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 32s 184ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 183ms/step\n",
            ">>>> FOLD 4 Jaccard = 0.6899594941867637\n",
            "\n",
            "#########################\n",
            "### FOLD 5\n",
            "#########################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFRobertaModel were initialized from the model checkpoint at roberta-large.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "688/688 [==============================] - 284s 412ms/step - loss: 3.3256 - activation_loss: 1.6787 - activation_1_loss: 1.6469 - val_loss: 2.7357 - val_activation_loss: 1.3740 - val_activation_1_loss: 1.3617\n",
            "Epoch 2/2\n",
            "688/688 [==============================] - 261s 379ms/step - loss: 2.6967 - activation_loss: 1.3614 - activation_1_loss: 1.3353 - val_loss: 2.5256 - val_activation_loss: 1.2787 - val_activation_1_loss: 1.2469\n",
            "Epoch 3/3\n",
            "688/688 [==============================] - 270s 393ms/step - loss: 2.5657 - activation_loss: 1.3004 - activation_1_loss: 1.2654 - val_loss: 2.5299 - val_activation_loss: 1.2967 - val_activation_1_loss: 1.2333\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 32s 185ms/step\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 20s 184ms/step\n",
            ">>>> FOLD 5 Jaccard = 0.6935789611642874\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LuPiV-HdLZn",
        "colab_type": "text"
      },
      "source": [
        "#### Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf-xLJB2dLZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all = []\n",
        "for k in range(input_ids_t.shape[0]):\n",
        "    a = np.argmax(preds_start[k,])\n",
        "    b = np.argmax(preds_end[k,])\n",
        "    if a>b: \n",
        "        st = test.loc[k,'text']\n",
        "    else:\n",
        "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "        enc = tokenizer.encode(text1)\n",
        "        #print(text1)\n",
        "        #print(enc)\n",
        "        st = tokenizer.decode(enc.ids[a-2:b-1])\n",
        "    all.append(st)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uVGBxD9dLZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "outputId": "e55cac7e-d189-4e2f-b539-ed57fefaea6f"
      },
      "source": [
        "test['selected_text'] = all\n",
        "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
        "pd.set_option('max_colwidth', 60)\n",
        "test.sample(25)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>selected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>12005b65fc</td>\n",
              "      <td>Waiting for my turn on wii fit gym closed</td>\n",
              "      <td>neutral</td>\n",
              "      <td>waiting for my turn on wii fit gym closed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>661</th>\n",
              "      <td>bcf13877f7</td>\n",
              "      <td>Good morning everyone</td>\n",
              "      <td>positive</td>\n",
              "      <td>good morning everyone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1968</th>\n",
              "      <td>575e4a89fe</td>\n",
              "      <td>tts ridiculously sweet of you</td>\n",
              "      <td>positive</td>\n",
              "      <td>ridiculously sweet of you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>826</th>\n",
              "      <td>a0b1828b67</td>\n",
              "      <td>'Brides a la mode' pow wow first thing this morning   Th...</td>\n",
              "      <td>positive</td>\n",
              "      <td>lovely</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992</th>\n",
              "      <td>472c3e2c41</td>\n",
              "      <td>Getting somewhere with my first 'real' KiokuDB and catal...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>getting somewhere with my first 'real' kiokudb and cata...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1922</th>\n",
              "      <td>ce71d002ec</td>\n",
              "      <td>Mommas day is may 10th! Don`t forget to do something nic...</td>\n",
              "      <td>positive</td>\n",
              "      <td>nice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1043</th>\n",
              "      <td>8db4aaef4a</td>\n",
              "      <td>watching the notebook</td>\n",
              "      <td>neutral</td>\n",
              "      <td>watching the notebook</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2111</th>\n",
              "      <td>895de1648c</td>\n",
              "      <td>really tired. and have to work the whole day tomorrow, t...</td>\n",
              "      <td>negative</td>\n",
              "      <td>depresses</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1695</th>\n",
              "      <td>78d89e7c64</td>\n",
              "      <td>Yeah prbly pickin up songs for SingStar. Haven`t checke...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>yeah prbly pickin up songs for singstar. haven`t checke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2220</th>\n",
              "      <td>756d255e40</td>\n",
              "      <td>is at home with a pukey boy! Poor little baby</td>\n",
              "      <td>negative</td>\n",
              "      <td>poor little baby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2743</th>\n",
              "      <td>d4aa6a714a</td>\n",
              "      <td>Sitting in a shadow of the tree in the heart of the city...</td>\n",
              "      <td>positive</td>\n",
              "      <td>thanks,wind,for being so pleasant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2290</th>\n",
              "      <td>c9a52dee1f</td>\n",
              "      <td>Guess I`m gonna try the nap thing again 2day, but since ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>haven`t cooperated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3166</th>\n",
              "      <td>99fdaff40d</td>\n",
              "      <td>http://twitpic.com/4wi9p - playing with ethan. i love yo...</td>\n",
              "      <td>positive</td>\n",
              "      <td>i love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1277</th>\n",
              "      <td>337ebe2747</td>\n",
              "      <td>FRIDAY so freakin happy today was an annoying day  buuut...</td>\n",
              "      <td>positive</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1257</th>\n",
              "      <td>6816923abd</td>\n",
              "      <td>This month was a bad month to try and get an advert toge...</td>\n",
              "      <td>negative</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2241</th>\n",
              "      <td>982b1c05d3</td>\n",
              "      <td>LOVE the album guys and can`t wait for the official rel...</td>\n",
              "      <td>positive</td>\n",
              "      <td>love</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2343</th>\n",
              "      <td>fc5de3287d</td>\n",
              "      <td>: nooo  i don`t know why...i click on TweetDeck_0_25_ma...</td>\n",
              "      <td>negative</td>\n",
              "      <td>so sad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2892</th>\n",
              "      <td>f79609f1b3</td>\n",
              "      <td>Lovin` , , &amp; _Bailon SOOOO much right now!</td>\n",
              "      <td>neutral</td>\n",
              "      <td>lovin` , , &amp; _bailon soooo much right now!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>af0778ed50</td>\n",
              "      <td>should not have waled past the Quad! I want to be outsid...</td>\n",
              "      <td>positive</td>\n",
              "      <td>fun.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2418</th>\n",
              "      <td>49c713c76d</td>\n",
              "      <td>Is It The Bit Where Hollie Started Crying?</td>\n",
              "      <td>neutral</td>\n",
              "      <td>is it the bit where hollie started crying?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>867</th>\n",
              "      <td>e5a26fb323</td>\n",
              "      <td>lol! woow okay its not that big of a deal</td>\n",
              "      <td>neutral</td>\n",
              "      <td>lol! woow okay its not that big of a deal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>1086c26e82</td>\n",
              "      <td>http://twitpic.com/4vuuy - That`s so cool.</td>\n",
              "      <td>positive</td>\n",
              "      <td>that`s so cool.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1472</th>\n",
              "      <td>1f9019bba5</td>\n",
              "      <td>lmao you witty wacko...loves it</td>\n",
              "      <td>positive</td>\n",
              "      <td>loves</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1930</th>\n",
              "      <td>a267c3e874</td>\n",
              "      <td>We are at alexander. Just had a 3 course dinner and i a...</td>\n",
              "      <td>neutral</td>\n",
              "      <td>we are at alexander. just had a 3 course dinner and i a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>ea1b1fd1bb</td>\n",
              "      <td>Mudweight hauled in for last time by   http://yfrog.com/...</td>\n",
              "      <td>negative</td>\n",
              "      <td>mudweight hauled in for last time</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          textID  ...                                                selected_text\n",
              "1191  12005b65fc  ...                    waiting for my turn on wii fit gym closed\n",
              "661   bcf13877f7  ...                                        good morning everyone\n",
              "1968  575e4a89fe  ...                                    ridiculously sweet of you\n",
              "826   a0b1828b67  ...                                                       lovely\n",
              "992   472c3e2c41  ...   getting somewhere with my first 'real' kiokudb and cata...\n",
              "1922  ce71d002ec  ...                                                         nice\n",
              "1043  8db4aaef4a  ...                                        watching the notebook\n",
              "2111  895de1648c  ...                                                    depresses\n",
              "1695  78d89e7c64  ...   yeah prbly pickin up songs for singstar. haven`t checke...\n",
              "2220  756d255e40  ...                                             poor little baby\n",
              "2743  d4aa6a714a  ...                            thanks,wind,for being so pleasant\n",
              "2290  c9a52dee1f  ...                                           haven`t cooperated\n",
              "3166  99fdaff40d  ...                                                       i love\n",
              "1277  337ebe2747  ...                                                        happy\n",
              "1257  6816923abd  ...                                                          bad\n",
              "2241  982b1c05d3  ...                                                         love\n",
              "2343  fc5de3287d  ...                                                    so sad...\n",
              "2892  f79609f1b3  ...                   lovin` , , & _bailon soooo much right now!\n",
              "254   af0778ed50  ...                                                         fun.\n",
              "2418  49c713c76d  ...                   is it the bit where hollie started crying?\n",
              "867   e5a26fb323  ...                    lol! woow okay its not that big of a deal\n",
              "309   1086c26e82  ...                                              that`s so cool.\n",
              "1472  1f9019bba5  ...                                                        loves\n",
              "1930  a267c3e874  ...   we are at alexander. just had a 3 course dinner and i a...\n",
              "1243  ea1b1fd1bb  ...                            mudweight hauled in for last time\n",
              "\n",
              "[25 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}